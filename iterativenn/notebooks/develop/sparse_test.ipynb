{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7f4d4e-bd21-4e1c-be95-e58f1bcc743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce9a10d9",
   "metadata": {},
   "source": [
    "Here is a canonical method of creating a sparse tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d695c353-55dc-4911-96c6-adad7baca691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createS():\n",
    "    # These are the indices.  They need to be integers.\n",
    "    # 2 rows and n columns\n",
    "    i = torch.tensor([[0, 1, 1],\n",
    "                      [2, 0, 2]])\n",
    "    # n scalars in a vector with which to fill in the sparse matrix\n",
    "    v = torch.tensor([3.0, 4.0, 5.0])\n",
    "    # This needs i, v, and an explicit size, since you can't\n",
    "    # infer the size (some row or column might be all zero)\n",
    "    S = torch.sparse_coo_tensor(i, v, (2, 3),\n",
    "                                requires_grad=True)\n",
    "    # Note, there is no detach here on purpose.\n",
    "    # It raises an error otherwise:\n",
    "    # RuntimeError: set_indices_and_values_unsafe is not allowed on a Tensor created from .data or .detach().\n",
    "    # If your intent is to change the metadata of a Tensor (such as sizes / strides / storage / storage_offset)\n",
    "    # without autograd tracking the change, remove the .data / .detach() call and wrap the change in a `with torch.no_grad():` block.\n",
    "    # For example, change:\n",
    "    #   x.data.set_(y)\n",
    "    # to:\n",
    "    # with torch.no_grad():\n",
    "    #     x.set_(y)\n",
    "    with torch.no_grad():\n",
    "        # Not working version\n",
    "        # S = S.coalesce().clone().detach().requires_grad_(True)\n",
    "        S = S.coalesce().clone().requires_grad_(True)\n",
    "    return S"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbc9017c",
   "metadata": {},
   "source": [
    "We will eventually want an input, and a target since our equation will be\n",
    "something like:\n",
    "\n",
    "$$\n",
    "S b = y\n",
    "$$\n",
    "\n",
    "Note, they can be dense since they are just 1-D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e6707b-17d7-4b1b-88f0-50dc47282c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[3.0], [4.0], [5.0]],\n",
    "                 requires_grad=False)\n",
    "\n",
    "y = torch.tensor([[2.0], [1.0]],\n",
    "                 requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666107ef-398a-4e63-ac1a-27c9dc9f6711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.],\n",
       "        [37.]], grad_fn=<SparseAddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a sparse dot product you can take a gradient\n",
    "# of.  See this page for details:\n",
    "# https://pytorch.org/docs/stable/sparse.html\n",
    "S = createS()\n",
    "yHat = torch.sparse.mm(S, b)\n",
    "yHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec2d9bd-3417-41e5-9111-f6f0db9b760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(34.9803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(31.6903, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(28.4059, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(25.1283, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.8586, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(18.5986, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(15.3509, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(12.1193, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(8.9105, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is the simplest possible example of a gradient descent with all of the\n",
    "# pieces pulled apart.\n",
    "S = createS()\n",
    "for i in range(10):\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "\n",
    "    yHat = torch.sparse.mm(S, b)\n",
    "    loss = torch.norm(y-yHat)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gamma = 0.1\n",
    "        S -= gamma*S.grad\n",
    "        # Note\n",
    "        #    S = S - gamma*S.grad\n",
    "        # does not work since S will then be a new copy and not in place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614957f9-3ef5-439d-95df-d433eb137cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4394, 2.6187, 4.3645])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.grad.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc1cf25-b2b4-4a7e-9e1c-c69e0e9121c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(34.9803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(31.6903, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(28.4059, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(25.1283, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.8586, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(18.5986, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(15.3509, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(12.1193, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(8.9105, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we be a bit fancier and use an optimizer.  This is identical to the above\n",
    "S = createS()\n",
    "optimizer = torch.optim.SGD([S], lr=0.1)\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    yHat = torch.sparse.mm(S, b)\n",
    "    loss = torch.norm(y-yHat)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b03d75-eca5-44b2-9bf0-2693ac530cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(34.9803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(31.3612, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(27.7163, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(24.0766, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.4472, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(16.8311, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(13.2328, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(9.6599, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(6.1281, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's play around with momentum\n",
    "S = createS()\n",
    "# The velocity vector for the optimization\n",
    "V = createS()*0\n",
    "for i in range(10):\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "\n",
    "    yHat = torch.sparse.mm(S, b)\n",
    "    loss = torch.norm(y-yHat)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gamma = 0.1\n",
    "        mu = 0.1\n",
    "        # This momentum implementation is from\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "        V = mu*V + S.grad\n",
    "        S -= gamma*V\n",
    "        # Note\n",
    "        #    S = S - gamma*S.grad\n",
    "        # does not work since S will then be a new copy and not in place!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96a70020-fa01-4924-a718-fc42e763218b",
   "metadata": {},
   "source": [
    "Note, this one does not work! This is because pytorch optimizers are not really setup\n",
    "for running sparse tensors... at least the way I think of them.\n",
    "\n",
    "- https://github.com/pytorch/pytorch/issues/1285\n",
    "- https://github.com/pytorch/pytorch/issues/1369\n",
    "- https://github.com/pytorch/pytorch/issues/9674\n",
    "- https://github.com/pytorch/pytorch/issues/10043\n",
    "\n",
    "Here is a search for all of the discussion\n",
    "\n",
    "https://github.com/pytorch/pytorch/labels/module%3A%20sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "752fb473",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "memory format option is only supported by strided tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mmm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS, b)\n\u001b[1;32m     24\u001b[0m my_model \u001b[39m=\u001b[39m SparseModel(S, b)\n\u001b[0;32m---> 27\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdagrad(my_model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39m# optimizer = torch.optim.SparseAdam(my_model.parameters(), lr=0.1)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/projects/iterativenn/venv/lib/python3.9/site-packages/torch/optim/adagrad.py:62\u001b[0m, in \u001b[0;36mAdagrad.__init__\u001b[0;34m(self, params, lr, lr_decay, weight_decay, initial_accumulator_value, eps, foreach, maximize, differentiable)\u001b[0m\n\u001b[1;32m     56\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m)\n\u001b[1;32m     57\u001b[0m init_value \u001b[39m=\u001b[39m (\n\u001b[1;32m     58\u001b[0m     \u001b[39mcomplex\u001b[39m(initial_accumulator_value, initial_accumulator_value)\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(p)\n\u001b[1;32m     60\u001b[0m     \u001b[39melse\u001b[39;00m initial_accumulator_value\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfull_like(\n\u001b[1;32m     63\u001b[0m     p, init_value, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format\n\u001b[1;32m     64\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: memory format option is only supported by strided tensors"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([[3.0], [4.0], [5.0]],\n",
    "                 requires_grad=False)\n",
    "\n",
    "y = torch.tensor([[2.0], [1.0]],\n",
    "                 requires_grad=False)\n",
    "\n",
    "i = torch.tensor([[0, 1, 1],\n",
    "                  [2, 0, 2]])\n",
    "# n scalars in a vector with which to fill in the sparse matrix\n",
    "v = torch.tensor([3.0, 4.0, 5.0])\n",
    "# This needs i, v, and an explicit size, since you can't\n",
    "# infer the size (some row or column might be all zero)\n",
    "S = torch.sparse_coo_tensor(i, v, (2, 3),\n",
    "                            requires_grad=True).coalesce()\n",
    "\n",
    "class SparseModel(torch.nn.Module):\n",
    "    def __init__(self, S, b):\n",
    "        super().__init__()\n",
    "        self.S = torch.nn.Parameter(S)\n",
    "\n",
    "    def forward(self, b):\n",
    "        return torch.sparse.mm(self.S, b)\n",
    "    \n",
    "my_model = SparseModel(S, b)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
    "# optimizer = torch.optim.SparseAdam(my_model.parameters(), lr=0.1)\n",
    "# optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "yHat = my_model(b)\n",
    "loss = torch.norm(y-yHat)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b838338-1273-4c21-9216-4e1e10a9de28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "memory format option is only supported by strided tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m make_error:\n\u001b[1;32m      3\u001b[0m     S \u001b[39m=\u001b[39m createS()\n\u001b[0;32m----> 4\u001b[0m     optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdagrad([S], lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      6\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/projects/iterativenn/venv/lib/python3.9/site-packages/torch/optim/adagrad.py:62\u001b[0m, in \u001b[0;36mAdagrad.__init__\u001b[0;34m(self, params, lr, lr_decay, weight_decay, initial_accumulator_value, eps, foreach, maximize, differentiable)\u001b[0m\n\u001b[1;32m     56\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m)\n\u001b[1;32m     57\u001b[0m init_value \u001b[39m=\u001b[39m (\n\u001b[1;32m     58\u001b[0m     \u001b[39mcomplex\u001b[39m(initial_accumulator_value, initial_accumulator_value)\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(p)\n\u001b[1;32m     60\u001b[0m     \u001b[39melse\u001b[39;00m initial_accumulator_value\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfull_like(\n\u001b[1;32m     63\u001b[0m     p, init_value, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format\n\u001b[1;32m     64\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: memory format option is only supported by strided tensors"
     ]
    }
   ],
   "source": [
    "make_error = True\n",
    "if make_error:\n",
    "    S = createS()\n",
    "    optimizer = torch.optim.Adagrad([S], lr=0.1)\n",
    "    for i in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        yHat = torch.sparse.mm(S, b)\n",
    "        print(b)\n",
    "        loss = torch.norm(y-yHat)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f995a07-d115-4bfb-a368-a5dcd0dec1d3",
   "metadata": {},
   "source": [
    "Using my little trick.  Takes more memory, but allows optimizers to work\n",
    "\n",
    "$U$ is an update that you can train, but it goes through a mask $M$ which you can't train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4ef754-bba8-40cb-b3bd-7f89a2c9a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.8661, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(14.1485, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(13.4359, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(12.7292, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(12.0295, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(11.3382, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(10.6569, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(9.9876, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(9.3328, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(8.6955, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor([[-1.0066,  0.0000,  0.0000],\n",
      "        [-0.9901, -0.9901,  0.0000]], requires_grad=True)\n",
      "tensor([[-0.0066,  1.0000,  1.0000],\n",
      "        [ 0.0099,  0.0099,  1.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Updates = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], requires_grad=True)\n",
    "Omega = torch.tensor([[1, 0, 0], [1 , 1, 0]], requires_grad=False)\n",
    "Init = torch.tensor([[1, 1, 1], [1, 1, 1]], requires_grad=False)\n",
    "\n",
    "optimizer = torch.optim.Adam([Updates], lr=0.1)\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    W = Init+Omega*Updates\n",
    "    yHat = W @ b\n",
    "    loss = torch.norm(y-yHat)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "print(Updates)\n",
    "print(Init+Omega*Updates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40abada5-d79a-49a8-acce-4efa992a4af0",
   "metadata": {},
   "source": [
    "Copy the Pytorch RMSprop and see if I can get it to work with a sparse tensor\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\n",
    "\n",
    "Here is the raw code\n",
    "https://raw.githubusercontent.com/pytorch/pytorch/master/torch/optim/rmsprop.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a9a2e0-0a88-4623-ba13-cc5e9b08b3af",
   "metadata": {},
   "source": [
    "Here is my slightly modified code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8c6c1a-3779-446c-b028-d84c1b616ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class SparseRMSprop(Optimizer):\n",
    "    r\"\"\"Implements RMSprop algorithm.\n",
    "\n",
    "    .. math::\n",
    "       \\begin{aligned}\n",
    "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "            &\\textbf{input}      : \\alpha \\text{ (alpha)},\\: \\gamma \\text{ (lr)},\n",
    "                \\: \\theta_0 \\text{ (params)}, \\: f(\\theta) \\text{ (objective)}                   \\\\\n",
    "            &\\hspace{13mm}   \\lambda \\text{ (weight decay)},\\: \\mu \\text{ (momentum)},\\: centered\\\\\n",
    "            &\\textbf{initialize} : v_0 \\leftarrow 0 \\text{ (square average)}, \\:\n",
    "                \\textbf{b}_0 \\leftarrow 0 \\text{ (buffer)}, \\: g^{ave}_0 \\leftarrow 0     \\\\[-1.ex]\n",
    "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
    "            &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
    "            &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n",
    "            &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
    "            &\\hspace{5mm}v_t           \\leftarrow   \\alpha v_{t-1} + (1 - \\alpha) g^2_t\n",
    "                \\hspace{8mm}                                                                     \\\\\n",
    "            &\\hspace{5mm} \\tilde{v_t} \\leftarrow v_t                                             \\\\\n",
    "            &\\hspace{5mm}if \\: centered                                                          \\\\\n",
    "            &\\hspace{10mm} g^{ave}_t \\leftarrow g^{ave}_{t-1} \\alpha + (1-\\alpha) g_t            \\\\\n",
    "            &\\hspace{10mm} \\tilde{v_t} \\leftarrow \\tilde{v_t} -  \\big(g^{ave}_{t} \\big)^2        \\\\\n",
    "            &\\hspace{5mm}if \\: \\mu > 0                                                           \\\\\n",
    "            &\\hspace{10mm} \\textbf{b}_t\\leftarrow \\mu \\textbf{b}_{t-1} +\n",
    "                g_t/ \\big(\\sqrt{\\tilde{v_t}} +  \\epsilon \\big)                                   \\\\\n",
    "            &\\hspace{10mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\textbf{b}_t                \\\\\n",
    "            &\\hspace{5mm} else                                                                   \\\\\n",
    "            &\\hspace{10mm}\\theta_t      \\leftarrow   \\theta_{t-1} -\n",
    "                \\gamma  g_t/ \\big(\\sqrt{\\tilde{v_t}} + \\epsilon \\big)  \\hspace{3mm}              \\\\\n",
    "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
    "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "       \\end{aligned}\n",
    "\n",
    "    For further details regarding the algorithm we refer to\n",
    "    `lecture notes <https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_ by G. Hinton.\n",
    "    and centered version `Generating Sequences\n",
    "    With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n",
    "    The implementation here takes the square root of the gradient average before\n",
    "    adding epsilon (note that TensorFlow interchanges these two operations). The effective\n",
    "    learning rate is thus :math:`\\gamma/(\\sqrt{v} + \\epsilon)` where :math:`\\gamma`\n",
    "    is the scheduled learning rate and :math:`v` is the weighted moving average\n",
    "    of the squared gradient.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-2)\n",
    "        momentum (float, optional): momentum factor (default: 0)\n",
    "        alpha (float, optional): smoothing constant (default: 0.99)\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        centered (bool, optional) : if ``True``, compute the centered RMSProp,\n",
    "            the gradient is normalized by an estimation of its variance\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        foreach (bool, optional): whether foreach implementation of optimizer\n",
    "            is used (default: None)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0,\n",
    "                 centered=False, foreach: Optional[bool] = None):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= momentum:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if not 0.0 <= alpha:\n",
    "            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered,\n",
    "                        weight_decay=weight_decay, foreach=foreach)\n",
    "        super(SparseRMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('momentum', 0)\n",
    "            group.setdefault('centered', False)\n",
    "            group.setdefault('foreach', None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            square_avgs = []\n",
    "            grad_avgs = []\n",
    "            momentum_buffer_list = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # FIXME:  This is not quite what I intended, even though it works.  zeros_like of a sparse\n",
    "                    # matrix is an empty matrix.  It might be okay to just use zeros_like since it does the\n",
    "                    # right thing for dense matrices, and it just an empty matrix for sparse matrices.\n",
    "                    state['square_avg'] = torch.zeros_like(p)\n",
    "                    if group['momentum'] > 0:\n",
    "                        state['momentum_buffer'] = torch.zeros_like(p)\n",
    "                    if group['centered']:\n",
    "                        state['grad_avg'] = torch.zeros_like(p)\n",
    "\n",
    "                square_avgs.append(state['square_avg'])\n",
    "\n",
    "                if group['momentum'] > 0:\n",
    "                    momentum_buffer_list.append(state['momentum_buffer'])\n",
    "                if group['centered']:\n",
    "                    grad_avgs.append(state['grad_avg'])\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "\n",
    "            rmsprop(params_with_grad,\n",
    "                    grads,\n",
    "                    square_avgs,\n",
    "                    grad_avgs,\n",
    "                    momentum_buffer_list,\n",
    "                    lr=group['lr'],\n",
    "                    alpha=group['alpha'],\n",
    "                    eps=group['eps'],\n",
    "                    weight_decay=group['weight_decay'],\n",
    "                    momentum=group['momentum'],\n",
    "                    centered=group['centered'],\n",
    "                    foreach=group['foreach'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def rmsprop(params: List[Tensor],\n",
    "            grads: List[Tensor],\n",
    "            square_avgs: List[Tensor],\n",
    "            grad_avgs: List[Tensor],\n",
    "            momentum_buffer_list: List[Tensor],\n",
    "            # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "            # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "            foreach: bool = None,\n",
    "            *,\n",
    "            lr: float,\n",
    "            alpha: float,\n",
    "            eps: float,\n",
    "            weight_decay: float,\n",
    "            momentum: float,\n",
    "            centered: bool):\n",
    "    r\"\"\"Functional API that performs rmsprop algorithm computation.\n",
    "    See :class:`~torch.optim.RMSProp` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    if foreach is None:\n",
    "        # Placeholder for more complex foreach logic to be added when value is not set\n",
    "        foreach = False\n",
    "\n",
    "    assert not foreach, 'Not supported'\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_rmsprop\n",
    "    else:\n",
    "        func = _single_tensor_rmsprop\n",
    "\n",
    "    func(params,\n",
    "         grads,\n",
    "         square_avgs,\n",
    "         grad_avgs,\n",
    "         momentum_buffer_list,\n",
    "         lr=lr,\n",
    "         alpha=alpha,\n",
    "         eps=eps,\n",
    "         weight_decay=weight_decay,\n",
    "         momentum=momentum,\n",
    "         centered=centered)\n",
    "\n",
    "\n",
    "def _single_tensor_rmsprop(params: List[Tensor],\n",
    "                           grads: List[Tensor],\n",
    "                           square_avgs: List[Tensor],\n",
    "                           grad_avgs: List[Tensor],\n",
    "                           momentum_buffer_list: List[Tensor],\n",
    "                           *,\n",
    "                           lr: float,\n",
    "                           alpha: float,\n",
    "                           eps: float,\n",
    "                           weight_decay: float,\n",
    "                           momentum: float,\n",
    "                           centered: bool):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        square_avg = square_avgs[i]\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            grad = grad.add(param, alpha=weight_decay)\n",
    "\n",
    "        if square_avg.is_sparse:\n",
    "            square_avg *= alpha\n",
    "            square_avg += grad*grad*(1-alpha)\n",
    "        else:\n",
    "            square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "\n",
    "        if centered:\n",
    "            grad_avg = grad_avgs[i]\n",
    "            assert not grad_avg.is_sparse, 'Not supported'\n",
    "            grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)\n",
    "            avg = square_avg.addcmul(grad_avg, grad_avg, value=-1).sqrt_().add_(eps)\n",
    "        else:\n",
    "            if square_avg.is_sparse:\n",
    "                avg = square_avg.sqrt()\n",
    "                # Note: tmp is a pointer to the same memory as the values of avg, so updating\n",
    "                # tmp propagates back to \n",
    "                tmp = avg.values()\n",
    "                tmp += eps\n",
    "            else:\n",
    "                avg = square_avg.sqrt().add_(eps)\n",
    "\n",
    "        if momentum > 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "            assert not buf.is_sparse, 'Not supported'\n",
    "            buf.mul_(momentum).addcdiv_(grad, avg)\n",
    "            param.add_(buf, alpha=-lr)\n",
    "        else:\n",
    "            # This needs some explanation. These are copies\n",
    "            # of the original sparse tensors, but just the data\n",
    "            # part.  They are also \"coalesce\" versions as in:\n",
    "            # https://pytorch.org/docs/stable/sparse.html\n",
    "            # Of course, this does not work with non-coalesced\n",
    "            # vectors.\n",
    "            if param.is_sparse:\n",
    "                grad_v = grad.coalesce().clone().detach().values()\n",
    "                avg_v = avg.coalesce().clone().detach().values()\n",
    "                # These need to be coalesced, otherwise this is not defined.\n",
    "                grad_v /= avg_v\n",
    "                param += -lr*grad\n",
    "            else:\n",
    "                param.addcdiv_(grad, avg, value=-lr)\n",
    "\n",
    "def _multi_tensor_rmsprop(params: List[Tensor],\n",
    "                          grads: List[Tensor],\n",
    "                          square_avgs: List[Tensor],\n",
    "                          grad_avgs: List[Tensor],\n",
    "                          momentum_buffer_list: List[Tensor],\n",
    "                          *,\n",
    "                          lr: float,\n",
    "                          alpha: float,\n",
    "                          eps: float,\n",
    "                          weight_decay: float,\n",
    "                          momentum: float,\n",
    "                          centered: bool):\n",
    "\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    if weight_decay != 0:\n",
    "        torch._foreach_add_(grads, params, alpha=weight_decay)\n",
    "\n",
    "    torch._foreach_mul_(square_avgs, alpha)\n",
    "    torch._foreach_addcmul_(square_avgs, grads, grads, value=1 - alpha)\n",
    "\n",
    "    if centered:\n",
    "        torch._foreach_mul_(grad_avgs, alpha)\n",
    "        torch._foreach_add_(grad_avgs, grads, alpha=1 - alpha)\n",
    "        avg = torch._foreach_addcmul(square_avgs, grad_avgs, grad_avgs, value=-1)\n",
    "        torch._foreach_sqrt_(avg)\n",
    "        torch._foreach_add_(avg, eps)\n",
    "    else:\n",
    "        avg = torch._foreach_sqrt(square_avgs)\n",
    "        torch._foreach_add_(avg, eps)\n",
    "\n",
    "    if momentum > 0:\n",
    "        torch._foreach_mul_(momentum_buffer_list, momentum)\n",
    "        torch._foreach_addcdiv_(momentum_buffer_list, grads, avg)\n",
    "        torch._foreach_add_(params, momentum_buffer_list, alpha=-lr)\n",
    "    else:\n",
    "        torch._foreach_addcdiv_(params, grads, avg, value=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b0cec4-5a84-443e-a22d-e41e024e2d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(34.9803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(31.6903, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(28.4059, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(25.1283, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.8586, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(18.5986, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(15.3509, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(12.1193, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(8.9105, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we be a bit fancier and use an optimizer.  This is identical to the above\n",
    "S = createS()\n",
    "optimizer = SparseRMSprop([S], lr=0.1, momentum=0.00)\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    yHat = torch.sparse.mm(S, b)\n",
    "    loss = torch.norm(y-yHat)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c49bd19d-17c3-4787-afa6-66d921a8aa0a",
   "metadata": {},
   "source": [
    "FIXME:  Looking above this is the same answer as the SGD in all digits!  I must have broken the RMSProp optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ea238d-bed5-46f6-beae-c547fd11073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([4.6982, 6.8217, 9.7028]),\n",
      "       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
      "tensor([3., 4., 5.], requires_grad=True)\n",
      "tensor([4., 5., 6.], requires_grad=True)\n",
      "tensor([0.5000, 0.5000, 0.5000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Now we be a bit fancier and use an optimizer.  This is identical to the above\n",
    "S1 = createS()\n",
    "# /tmp/ipykernel_9374/1282355247.py:4: UserWarning: To copy construct from a tensor, \n",
    "# it is recommended to use sourceTensor.clone().detach() or \n",
    "# sourceTensor.clone().detach().requires_grad_(True), \n",
    "# rather than torch.tensor(sourceTensor).\n",
    "# So, not this\n",
    "# S2 = torch.tensor(S1.coalesce(), requires_grad=True)\n",
    "# but this\n",
    "S2 = S1.coalesce().clone().detach().requires_grad_(True)\n",
    "# I am a little worried about the .clone() and memory usage, but \n",
    "# that is for another day.\n",
    "\n",
    "yHat = torch.sparse.mm(S2, b)\n",
    "loss = torch.norm(y-yHat)\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(S2 + S2.grad)\n",
    "    print(S2.values())\n",
    "    # Ok, this seems to work.  I am kind of surprised, but happy.\n",
    "    # It must be that values() gives a pointer to the internal data.  \n",
    "    # This makes sense from the efficiency point of view.\n",
    "    tmp = S2.values() \n",
    "    tmp += 1\n",
    "    print(S2.values())\n",
    "    # Ok, this also seems to work, likely for the same reason.\n",
    "    tmp /= tmp*2    \n",
    "    print(S2.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b157b-c471-4d76-aa3a-36f965175888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
