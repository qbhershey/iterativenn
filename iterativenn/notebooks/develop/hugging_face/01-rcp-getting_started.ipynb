{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],\n",
       "         [ 0.2282,  0.5511, -0.5092,  ...,  0.6421,  0.9541,  0.3192],\n",
       "         [-0.6953, -0.0619,  0.2788,  ...,  0.6506,  0.7039, -1.7798],\n",
       "         ...,\n",
       "         [ 0.0128,  0.6943,  0.2900,  ...,  0.1643, -0.6300, -0.5545],\n",
       "         [-0.4211,  0.1978, -0.1303,  ...,  0.6586,  0.0929,  0.4538],\n",
       "         [-0.4778,  0.2352, -0.1104,  ...,  0.0203, -0.0158, -0.0116]],\n",
       "\n",
       "        [[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],\n",
       "         [ 0.2282,  0.5511, -0.5092,  ...,  0.6421,  0.9541,  0.3192],\n",
       "         [ 0.8647, -1.0665,  0.4561,  ...,  0.9921,  0.4701,  0.9641],\n",
       "         ...,\n",
       "         [-0.0958, -0.5744,  0.2631,  ...,  0.2453, -0.3293,  0.1269],\n",
       "         [ 0.0363, -0.4691,  0.0207,  ...,  0.4041, -0.5956,  0.2059],\n",
       "         [ 0.0454, -0.3303,  0.2008,  ...,  0.4410, -0.5415,  0.1128]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ALIGN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ALIGN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ALL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ALTCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ALTCLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ASTConfig',\n",
       " 'ASTFeatureExtractor',\n",
       " 'ASTForAudioClassification',\n",
       " 'ASTModel',\n",
       " 'ASTPreTrainedModel',\n",
       " 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'AUTOFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'AUTOFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'Adafactor',\n",
       " 'AdamW',\n",
       " 'AdamWeightDecay',\n",
       " 'AdaptiveEmbedding',\n",
       " 'AddedToken',\n",
       " 'Agent',\n",
       " 'AlbertConfig',\n",
       " 'AlbertForMaskedLM',\n",
       " 'AlbertForMultipleChoice',\n",
       " 'AlbertForPreTraining',\n",
       " 'AlbertForQuestionAnswering',\n",
       " 'AlbertForSequenceClassification',\n",
       " 'AlbertForTokenClassification',\n",
       " 'AlbertModel',\n",
       " 'AlbertPreTrainedModel',\n",
       " 'AlbertTokenizer',\n",
       " 'AlbertTokenizerFast',\n",
       " 'AlignConfig',\n",
       " 'AlignModel',\n",
       " 'AlignPreTrainedModel',\n",
       " 'AlignProcessor',\n",
       " 'AlignTextConfig',\n",
       " 'AlignTextModel',\n",
       " 'AlignVisionConfig',\n",
       " 'AlignVisionModel',\n",
       " 'AltCLIPConfig',\n",
       " 'AltCLIPModel',\n",
       " 'AltCLIPPreTrainedModel',\n",
       " 'AltCLIPProcessor',\n",
       " 'AltCLIPTextConfig',\n",
       " 'AltCLIPTextModel',\n",
       " 'AltCLIPVisionConfig',\n",
       " 'AltCLIPVisionModel',\n",
       " 'AudioClassificationPipeline',\n",
       " 'AutoBackbone',\n",
       " 'AutoConfig',\n",
       " 'AutoFeatureExtractor',\n",
       " 'AutoImageProcessor',\n",
       " 'AutoModel',\n",
       " 'AutoModelForAudioClassification',\n",
       " 'AutoModelForAudioFrameClassification',\n",
       " 'AutoModelForAudioXVector',\n",
       " 'AutoModelForCTC',\n",
       " 'AutoModelForCausalLM',\n",
       " 'AutoModelForDepthEstimation',\n",
       " 'AutoModelForDocumentQuestionAnswering',\n",
       " 'AutoModelForImageClassification',\n",
       " 'AutoModelForImageSegmentation',\n",
       " 'AutoModelForInstanceSegmentation',\n",
       " 'AutoModelForMaskGeneration',\n",
       " 'AutoModelForMaskedImageModeling',\n",
       " 'AutoModelForMaskedLM',\n",
       " 'AutoModelForMultipleChoice',\n",
       " 'AutoModelForNextSentencePrediction',\n",
       " 'AutoModelForObjectDetection',\n",
       " 'AutoModelForPreTraining',\n",
       " 'AutoModelForQuestionAnswering',\n",
       " 'AutoModelForSemanticSegmentation',\n",
       " 'AutoModelForSeq2SeqLM',\n",
       " 'AutoModelForSequenceClassification',\n",
       " 'AutoModelForSpeechSeq2Seq',\n",
       " 'AutoModelForTableQuestionAnswering',\n",
       " 'AutoModelForTokenClassification',\n",
       " 'AutoModelForUniversalSegmentation',\n",
       " 'AutoModelForVideoClassification',\n",
       " 'AutoModelForVision2Seq',\n",
       " 'AutoModelForVisualQuestionAnswering',\n",
       " 'AutoModelForZeroShotImageClassification',\n",
       " 'AutoModelForZeroShotObjectDetection',\n",
       " 'AutoModelWithLMHead',\n",
       " 'AutoProcessor',\n",
       " 'AutoTokenizer',\n",
       " 'AutoformerConfig',\n",
       " 'AutoformerForPrediction',\n",
       " 'AutoformerModel',\n",
       " 'AutoformerPreTrainedModel',\n",
       " 'AutomaticSpeechRecognitionPipeline',\n",
       " 'AzureOpenAiAgent',\n",
       " 'BART_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BEIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BEIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BIGBIRD_PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BIGBIRD_PEGASUS_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BIG_BIRD_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BIG_BIRD_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BIOGPT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BIOGPT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BLENDERBOT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BLENDERBOT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BLENDERBOT_SMALL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BLENDERBOT_SMALL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BLIP_2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BLIP_2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BLOOM_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BLOOM_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BRIDGETOWER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BRIDGETOWER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BartConfig',\n",
       " 'BartForCausalLM',\n",
       " 'BartForConditionalGeneration',\n",
       " 'BartForQuestionAnswering',\n",
       " 'BartForSequenceClassification',\n",
       " 'BartModel',\n",
       " 'BartPretrainedModel',\n",
       " 'BartTokenizer',\n",
       " 'BartTokenizerFast',\n",
       " 'BarthezTokenizer',\n",
       " 'BarthezTokenizerFast',\n",
       " 'BartphoTokenizer',\n",
       " 'BasicTokenizer',\n",
       " 'BatchEncoding',\n",
       " 'BatchFeature',\n",
       " 'BeamScorer',\n",
       " 'BeamSearchScorer',\n",
       " 'BeitConfig',\n",
       " 'BeitFeatureExtractor',\n",
       " 'BeitForImageClassification',\n",
       " 'BeitForMaskedImageModeling',\n",
       " 'BeitForSemanticSegmentation',\n",
       " 'BeitImageProcessor',\n",
       " 'BeitModel',\n",
       " 'BeitPreTrainedModel',\n",
       " 'BertConfig',\n",
       " 'BertForMaskedLM',\n",
       " 'BertForMultipleChoice',\n",
       " 'BertForNextSentencePrediction',\n",
       " 'BertForPreTraining',\n",
       " 'BertForQuestionAnswering',\n",
       " 'BertForSequenceClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BertGenerationConfig',\n",
       " 'BertGenerationDecoder',\n",
       " 'BertGenerationEncoder',\n",
       " 'BertGenerationPreTrainedModel',\n",
       " 'BertGenerationTokenizer',\n",
       " 'BertJapaneseTokenizer',\n",
       " 'BertLMHeadModel',\n",
       " 'BertLayer',\n",
       " 'BertModel',\n",
       " 'BertPreTrainedModel',\n",
       " 'BertTokenizer',\n",
       " 'BertTokenizerFast',\n",
       " 'BertweetTokenizer',\n",
       " 'BigBirdConfig',\n",
       " 'BigBirdForCausalLM',\n",
       " 'BigBirdForMaskedLM',\n",
       " 'BigBirdForMultipleChoice',\n",
       " 'BigBirdForPreTraining',\n",
       " 'BigBirdForQuestionAnswering',\n",
       " 'BigBirdForSequenceClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'BigBirdLayer',\n",
       " 'BigBirdModel',\n",
       " 'BigBirdPegasusConfig',\n",
       " 'BigBirdPegasusForCausalLM',\n",
       " 'BigBirdPegasusForConditionalGeneration',\n",
       " 'BigBirdPegasusForQuestionAnswering',\n",
       " 'BigBirdPegasusForSequenceClassification',\n",
       " 'BigBirdPegasusModel',\n",
       " 'BigBirdPegasusPreTrainedModel',\n",
       " 'BigBirdPreTrainedModel',\n",
       " 'BigBirdTokenizer',\n",
       " 'BigBirdTokenizerFast',\n",
       " 'BioGptConfig',\n",
       " 'BioGptForCausalLM',\n",
       " 'BioGptForSequenceClassification',\n",
       " 'BioGptForTokenClassification',\n",
       " 'BioGptModel',\n",
       " 'BioGptPreTrainedModel',\n",
       " 'BioGptTokenizer',\n",
       " 'BitBackbone',\n",
       " 'BitConfig',\n",
       " 'BitForImageClassification',\n",
       " 'BitImageProcessor',\n",
       " 'BitModel',\n",
       " 'BitPreTrainedModel',\n",
       " 'BitsAndBytesConfig',\n",
       " 'BlenderbotConfig',\n",
       " 'BlenderbotForCausalLM',\n",
       " 'BlenderbotForConditionalGeneration',\n",
       " 'BlenderbotModel',\n",
       " 'BlenderbotPreTrainedModel',\n",
       " 'BlenderbotSmallConfig',\n",
       " 'BlenderbotSmallForCausalLM',\n",
       " 'BlenderbotSmallForConditionalGeneration',\n",
       " 'BlenderbotSmallModel',\n",
       " 'BlenderbotSmallPreTrainedModel',\n",
       " 'BlenderbotSmallTokenizer',\n",
       " 'BlenderbotSmallTokenizerFast',\n",
       " 'BlenderbotTokenizer',\n",
       " 'BlenderbotTokenizerFast',\n",
       " 'Blip2Config',\n",
       " 'Blip2ForConditionalGeneration',\n",
       " 'Blip2Model',\n",
       " 'Blip2PreTrainedModel',\n",
       " 'Blip2Processor',\n",
       " 'Blip2QFormerConfig',\n",
       " 'Blip2QFormerModel',\n",
       " 'Blip2VisionConfig',\n",
       " 'Blip2VisionModel',\n",
       " 'BlipConfig',\n",
       " 'BlipForConditionalGeneration',\n",
       " 'BlipForImageTextRetrieval',\n",
       " 'BlipForQuestionAnswering',\n",
       " 'BlipImageProcessor',\n",
       " 'BlipModel',\n",
       " 'BlipPreTrainedModel',\n",
       " 'BlipProcessor',\n",
       " 'BlipTextConfig',\n",
       " 'BlipTextModel',\n",
       " 'BlipVisionConfig',\n",
       " 'BlipVisionModel',\n",
       " 'BloomConfig',\n",
       " 'BloomForCausalLM',\n",
       " 'BloomForQuestionAnswering',\n",
       " 'BloomForSequenceClassification',\n",
       " 'BloomForTokenClassification',\n",
       " 'BloomModel',\n",
       " 'BloomPreTrainedModel',\n",
       " 'BloomTokenizerFast',\n",
       " 'BridgeTowerConfig',\n",
       " 'BridgeTowerForContrastiveLearning',\n",
       " 'BridgeTowerForImageAndTextRetrieval',\n",
       " 'BridgeTowerForMaskedLM',\n",
       " 'BridgeTowerImageProcessor',\n",
       " 'BridgeTowerModel',\n",
       " 'BridgeTowerPreTrainedModel',\n",
       " 'BridgeTowerProcessor',\n",
       " 'BridgeTowerTextConfig',\n",
       " 'BridgeTowerVisionConfig',\n",
       " 'ByT5Tokenizer',\n",
       " 'CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CANINE_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CANINE_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CHINESE_CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CHINESE_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CLAP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CLIPConfig',\n",
       " 'CLIPFeatureExtractor',\n",
       " 'CLIPImageProcessor',\n",
       " 'CLIPModel',\n",
       " 'CLIPPreTrainedModel',\n",
       " 'CLIPProcessor',\n",
       " 'CLIPSEG_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CLIPSEG_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CLIPSegConfig',\n",
       " 'CLIPSegForImageSegmentation',\n",
       " 'CLIPSegModel',\n",
       " 'CLIPSegPreTrainedModel',\n",
       " 'CLIPSegProcessor',\n",
       " 'CLIPSegTextConfig',\n",
       " 'CLIPSegTextModel',\n",
       " 'CLIPSegVisionConfig',\n",
       " 'CLIPSegVisionModel',\n",
       " 'CLIPTextConfig',\n",
       " 'CLIPTextModel',\n",
       " 'CLIPTextModelWithProjection',\n",
       " 'CLIPTokenizer',\n",
       " 'CLIPTokenizerFast',\n",
       " 'CLIPVisionConfig',\n",
       " 'CLIPVisionModel',\n",
       " 'CLIPVisionModelWithProjection',\n",
       " 'CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CLIP_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CODEGEN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CONDITIONAL_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CONDITIONAL_DETR_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CONFIG_MAPPING',\n",
       " 'CONFIG_NAME',\n",
       " 'CONVBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CONVNEXTV2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CONVNEXTV2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CONVNEXT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CONVNEXT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CPMANT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CPMANT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CTRLConfig',\n",
       " 'CTRLForSequenceClassification',\n",
       " 'CTRLLMHeadModel',\n",
       " 'CTRLModel',\n",
       " 'CTRLPreTrainedModel',\n",
       " 'CTRLTokenizer',\n",
       " 'CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CTRL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CVT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CVT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CamembertConfig',\n",
       " 'CamembertForCausalLM',\n",
       " 'CamembertForMaskedLM',\n",
       " 'CamembertForMultipleChoice',\n",
       " 'CamembertForQuestionAnswering',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CamembertModel',\n",
       " 'CamembertPreTrainedModel',\n",
       " 'CamembertTokenizer',\n",
       " 'CamembertTokenizerFast',\n",
       " 'CanineConfig',\n",
       " 'CanineForMultipleChoice',\n",
       " 'CanineForQuestionAnswering',\n",
       " 'CanineForSequenceClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'CanineLayer',\n",
       " 'CanineModel',\n",
       " 'CaninePreTrainedModel',\n",
       " 'CanineTokenizer',\n",
       " 'CharSpan',\n",
       " 'CharacterTokenizer',\n",
       " 'ChineseCLIPConfig',\n",
       " 'ChineseCLIPFeatureExtractor',\n",
       " 'ChineseCLIPImageProcessor',\n",
       " 'ChineseCLIPModel',\n",
       " 'ChineseCLIPPreTrainedModel',\n",
       " 'ChineseCLIPProcessor',\n",
       " 'ChineseCLIPTextConfig',\n",
       " 'ChineseCLIPTextModel',\n",
       " 'ChineseCLIPVisionConfig',\n",
       " 'ChineseCLIPVisionModel',\n",
       " 'ClapAudioConfig',\n",
       " 'ClapAudioModel',\n",
       " 'ClapAudioModelWithProjection',\n",
       " 'ClapConfig',\n",
       " 'ClapFeatureExtractor',\n",
       " 'ClapModel',\n",
       " 'ClapPreTrainedModel',\n",
       " 'ClapProcessor',\n",
       " 'ClapTextConfig',\n",
       " 'ClapTextModel',\n",
       " 'ClapTextModelWithProjection',\n",
       " 'CodeGenConfig',\n",
       " 'CodeGenForCausalLM',\n",
       " 'CodeGenModel',\n",
       " 'CodeGenPreTrainedModel',\n",
       " 'CodeGenTokenizer',\n",
       " 'CodeGenTokenizerFast',\n",
       " 'ConditionalDetrConfig',\n",
       " 'ConditionalDetrFeatureExtractor',\n",
       " 'ConditionalDetrForObjectDetection',\n",
       " 'ConditionalDetrForSegmentation',\n",
       " 'ConditionalDetrImageProcessor',\n",
       " 'ConditionalDetrModel',\n",
       " 'ConditionalDetrPreTrainedModel',\n",
       " 'ConstrainedBeamSearchScorer',\n",
       " 'Constraint',\n",
       " 'ConstraintListState',\n",
       " 'Conv1D',\n",
       " 'ConvBertConfig',\n",
       " 'ConvBertForMaskedLM',\n",
       " 'ConvBertForMultipleChoice',\n",
       " 'ConvBertForQuestionAnswering',\n",
       " 'ConvBertForSequenceClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'ConvBertLayer',\n",
       " 'ConvBertModel',\n",
       " 'ConvBertPreTrainedModel',\n",
       " 'ConvBertTokenizer',\n",
       " 'ConvBertTokenizerFast',\n",
       " 'ConvNextBackbone',\n",
       " 'ConvNextConfig',\n",
       " 'ConvNextFeatureExtractor',\n",
       " 'ConvNextForImageClassification',\n",
       " 'ConvNextImageProcessor',\n",
       " 'ConvNextModel',\n",
       " 'ConvNextPreTrainedModel',\n",
       " 'ConvNextV2Backbone',\n",
       " 'ConvNextV2Config',\n",
       " 'ConvNextV2ForImageClassification',\n",
       " 'ConvNextV2Model',\n",
       " 'ConvNextV2PreTrainedModel',\n",
       " 'Conversation',\n",
       " 'ConversationalPipeline',\n",
       " 'CpmAntConfig',\n",
       " 'CpmAntForCausalLM',\n",
       " 'CpmAntModel',\n",
       " 'CpmAntPreTrainedModel',\n",
       " 'CpmAntTokenizer',\n",
       " 'CpmTokenizer',\n",
       " 'CpmTokenizerFast',\n",
       " 'CsvPipelineDataFormat',\n",
       " 'CvtConfig',\n",
       " 'CvtForImageClassification',\n",
       " 'CvtModel',\n",
       " 'CvtPreTrainedModel',\n",
       " 'DATA2VEC_AUDIO_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DATA2VEC_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DATA2VEC_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DATA2VEC_VISION_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DATA2VEC_VISION_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DEBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DEBERTA_V2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DECISION_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DECISION_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DEFORMABLE_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DEFORMABLE_DETR_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DEIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DEIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DETA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DETA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DETR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DETR_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DINAT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DINAT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DONUT_SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DONUT_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DPRConfig',\n",
       " 'DPRContextEncoder',\n",
       " 'DPRContextEncoderTokenizer',\n",
       " 'DPRContextEncoderTokenizerFast',\n",
       " 'DPRPreTrainedModel',\n",
       " 'DPRPretrainedContextEncoder',\n",
       " 'DPRPretrainedQuestionEncoder',\n",
       " 'DPRPretrainedReader',\n",
       " 'DPRQuestionEncoder',\n",
       " 'DPRQuestionEncoderTokenizer',\n",
       " 'DPRQuestionEncoderTokenizerFast',\n",
       " 'DPRReader',\n",
       " 'DPRReaderOutput',\n",
       " 'DPRReaderTokenizer',\n",
       " 'DPRReaderTokenizerFast',\n",
       " 'DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DPR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DPTConfig',\n",
       " 'DPTFeatureExtractor',\n",
       " 'DPTForDepthEstimation',\n",
       " 'DPTForSemanticSegmentation',\n",
       " 'DPTImageProcessor',\n",
       " 'DPTModel',\n",
       " 'DPTPreTrainedModel',\n",
       " 'DPT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DPT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'Data2VecAudioConfig',\n",
       " 'Data2VecAudioForAudioFrameClassification',\n",
       " 'Data2VecAudioForCTC',\n",
       " 'Data2VecAudioForSequenceClassification',\n",
       " 'Data2VecAudioForXVector',\n",
       " 'Data2VecAudioModel',\n",
       " 'Data2VecAudioPreTrainedModel',\n",
       " 'Data2VecTextConfig',\n",
       " 'Data2VecTextForCausalLM',\n",
       " 'Data2VecTextForMaskedLM',\n",
       " 'Data2VecTextForMultipleChoice',\n",
       " 'Data2VecTextForQuestionAnswering',\n",
       " 'Data2VecTextForSequenceClassification',\n",
       " 'Data2VecTextForTokenClassification',\n",
       " 'Data2VecTextModel',\n",
       " 'Data2VecTextPreTrainedModel',\n",
       " 'Data2VecVisionConfig',\n",
       " 'Data2VecVisionForImageClassification',\n",
       " 'Data2VecVisionForSemanticSegmentation',\n",
       " 'Data2VecVisionModel',\n",
       " 'Data2VecVisionPreTrainedModel',\n",
       " 'DataCollator',\n",
       " 'DataCollatorForLanguageModeling',\n",
       " 'DataCollatorForPermutationLanguageModeling',\n",
       " 'DataCollatorForSOP',\n",
       " 'DataCollatorForSeq2Seq',\n",
       " 'DataCollatorForTokenClassification',\n",
       " 'DataCollatorForWholeWordMask',\n",
       " 'DataCollatorWithPadding',\n",
       " 'DataProcessor',\n",
       " 'DebertaConfig',\n",
       " 'DebertaForMaskedLM',\n",
       " 'DebertaForQuestionAnswering',\n",
       " 'DebertaForSequenceClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaModel',\n",
       " 'DebertaPreTrainedModel',\n",
       " 'DebertaTokenizer',\n",
       " 'DebertaTokenizerFast',\n",
       " 'DebertaV2Config',\n",
       " 'DebertaV2ForMaskedLM',\n",
       " 'DebertaV2ForMultipleChoice',\n",
       " 'DebertaV2ForQuestionAnswering',\n",
       " 'DebertaV2ForSequenceClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DebertaV2Model',\n",
       " 'DebertaV2PreTrainedModel',\n",
       " 'DebertaV2Tokenizer',\n",
       " 'DebertaV2TokenizerFast',\n",
       " 'DecisionTransformerConfig',\n",
       " 'DecisionTransformerGPT2Model',\n",
       " 'DecisionTransformerGPT2PreTrainedModel',\n",
       " 'DecisionTransformerModel',\n",
       " 'DecisionTransformerPreTrainedModel',\n",
       " 'DefaultDataCollator',\n",
       " 'DefaultFlowCallback',\n",
       " 'DeformableDetrConfig',\n",
       " 'DeformableDetrFeatureExtractor',\n",
       " 'DeformableDetrForObjectDetection',\n",
       " 'DeformableDetrImageProcessor',\n",
       " 'DeformableDetrModel',\n",
       " 'DeformableDetrPreTrainedModel',\n",
       " 'DeiTConfig',\n",
       " 'DeiTFeatureExtractor',\n",
       " 'DeiTForImageClassification',\n",
       " 'DeiTForImageClassificationWithTeacher',\n",
       " 'DeiTForMaskedImageModeling',\n",
       " 'DeiTImageProcessor',\n",
       " 'DeiTModel',\n",
       " 'DeiTPreTrainedModel',\n",
       " 'DepthEstimationPipeline',\n",
       " 'DetaConfig',\n",
       " 'DetaForObjectDetection',\n",
       " 'DetaImageProcessor',\n",
       " 'DetaModel',\n",
       " 'DetaPreTrainedModel',\n",
       " 'DetrConfig',\n",
       " 'DetrFeatureExtractor',\n",
       " 'DetrForObjectDetection',\n",
       " 'DetrForSegmentation',\n",
       " 'DetrImageProcessor',\n",
       " 'DetrModel',\n",
       " 'DetrPreTrainedModel',\n",
       " 'DinatBackbone',\n",
       " 'DinatConfig',\n",
       " 'DinatForImageClassification',\n",
       " 'DinatModel',\n",
       " 'DinatPreTrainedModel',\n",
       " 'DisjunctiveConstraint',\n",
       " 'DistilBertConfig',\n",
       " 'DistilBertForMaskedLM',\n",
       " 'DistilBertForMultipleChoice',\n",
       " 'DistilBertForQuestionAnswering',\n",
       " 'DistilBertForSequenceClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'DistilBertModel',\n",
       " 'DistilBertPreTrainedModel',\n",
       " 'DistilBertTokenizer',\n",
       " 'DistilBertTokenizerFast',\n",
       " 'DocumentQuestionAnsweringPipeline',\n",
       " 'DonutFeatureExtractor',\n",
       " 'DonutImageProcessor',\n",
       " 'DonutProcessor',\n",
       " 'DonutSwinConfig',\n",
       " 'DonutSwinModel',\n",
       " 'DonutSwinPreTrainedModel',\n",
       " 'DummyObject',\n",
       " 'EFFICIENTFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'EFFICIENTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'EFFICIENTNET_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'EFFICIENTNET_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ERNIE_M_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ERNIE_M_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ERNIE_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ERNIE_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ESM_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ESM_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'EarlyStoppingCallback',\n",
       " 'EfficientFormerConfig',\n",
       " 'EfficientFormerForImageClassification',\n",
       " 'EfficientFormerForImageClassificationWithTeacher',\n",
       " 'EfficientFormerImageProcessor',\n",
       " 'EfficientFormerModel',\n",
       " 'EfficientFormerPreTrainedModel',\n",
       " 'EfficientNetConfig',\n",
       " 'EfficientNetForImageClassification',\n",
       " 'EfficientNetImageProcessor',\n",
       " 'EfficientNetModel',\n",
       " 'EfficientNetPreTrainedModel',\n",
       " 'ElectraConfig',\n",
       " 'ElectraForCausalLM',\n",
       " 'ElectraForMaskedLM',\n",
       " 'ElectraForMultipleChoice',\n",
       " 'ElectraForPreTraining',\n",
       " 'ElectraForQuestionAnswering',\n",
       " 'ElectraForSequenceClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'ElectraModel',\n",
       " 'ElectraPreTrainedModel',\n",
       " 'ElectraTokenizer',\n",
       " 'ElectraTokenizerFast',\n",
       " 'EncoderDecoderConfig',\n",
       " 'EncoderDecoderModel',\n",
       " 'ErnieConfig',\n",
       " 'ErnieForCausalLM',\n",
       " 'ErnieForMaskedLM',\n",
       " 'ErnieForMultipleChoice',\n",
       " 'ErnieForNextSentencePrediction',\n",
       " 'ErnieForPreTraining',\n",
       " 'ErnieForQuestionAnswering',\n",
       " 'ErnieForSequenceClassification',\n",
       " 'ErnieForTokenClassification',\n",
       " 'ErnieMConfig',\n",
       " 'ErnieMForInformationExtraction',\n",
       " 'ErnieMForMultipleChoice',\n",
       " 'ErnieMForQuestionAnswering',\n",
       " 'ErnieMForSequenceClassification',\n",
       " 'ErnieMForTokenClassification',\n",
       " 'ErnieMModel',\n",
       " 'ErnieMPreTrainedModel',\n",
       " 'ErnieMTokenizer',\n",
       " 'ErnieModel',\n",
       " 'ErniePreTrainedModel',\n",
       " 'EsmConfig',\n",
       " 'EsmFoldPreTrainedModel',\n",
       " 'EsmForMaskedLM',\n",
       " 'EsmForProteinFolding',\n",
       " 'EsmForSequenceClassification',\n",
       " 'EsmForTokenClassification',\n",
       " 'EsmModel',\n",
       " 'EsmPreTrainedModel',\n",
       " 'EsmTokenizer',\n",
       " 'EvalPrediction',\n",
       " 'FEATURE_EXTRACTOR_MAPPING',\n",
       " 'FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FLAVA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FLAVA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FLAX_MODEL_FOR_CAUSAL_LM_MAPPING',\n",
       " 'FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING',\n",
       " 'FLAX_MODEL_FOR_MASKED_LM_MAPPING',\n",
       " 'FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING',\n",
       " 'FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING',\n",
       " 'FLAX_MODEL_FOR_PRETRAINING_MAPPING',\n",
       " 'FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING',\n",
       " 'FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING',\n",
       " 'FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING',\n",
       " 'FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING',\n",
       " 'FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING',\n",
       " 'FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING',\n",
       " 'FLAX_MODEL_MAPPING',\n",
       " 'FLAX_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FNET_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FNET_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FNetConfig',\n",
       " 'FNetForMaskedLM',\n",
       " 'FNetForMultipleChoice',\n",
       " 'FNetForNextSentencePrediction',\n",
       " 'FNetForPreTraining',\n",
       " 'FNetForQuestionAnswering',\n",
       " 'FNetForSequenceClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FNetLayer',\n",
       " 'FNetModel',\n",
       " 'FNetPreTrainedModel',\n",
       " 'FNetTokenizer',\n",
       " 'FNetTokenizerFast',\n",
       " 'FOCALNET_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FOCALNET_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FSMTConfig',\n",
       " 'FSMTForConditionalGeneration',\n",
       " 'FSMTModel',\n",
       " 'FSMTTokenizer',\n",
       " 'FSMT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FUNNEL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FeatureExtractionMixin',\n",
       " 'FeatureExtractionPipeline',\n",
       " 'FillMaskPipeline',\n",
       " 'FlaubertConfig',\n",
       " 'FlaubertForMultipleChoice',\n",
       " 'FlaubertForQuestionAnswering',\n",
       " 'FlaubertForQuestionAnsweringSimple',\n",
       " 'FlaubertForSequenceClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FlaubertModel',\n",
       " 'FlaubertPreTrainedModel',\n",
       " 'FlaubertTokenizer',\n",
       " 'FlaubertWithLMHeadModel',\n",
       " 'FlavaConfig',\n",
       " 'FlavaFeatureExtractor',\n",
       " 'FlavaForPreTraining',\n",
       " 'FlavaImageCodebook',\n",
       " 'FlavaImageCodebookConfig',\n",
       " 'FlavaImageConfig',\n",
       " 'FlavaImageModel',\n",
       " 'FlavaImageProcessor',\n",
       " 'FlavaModel',\n",
       " 'FlavaMultimodalConfig',\n",
       " 'FlavaMultimodalModel',\n",
       " 'FlavaPreTrainedModel',\n",
       " 'FlavaProcessor',\n",
       " 'FlavaTextConfig',\n",
       " 'FlavaTextModel',\n",
       " 'FlaxAlbertForMaskedLM',\n",
       " 'FlaxAlbertForMultipleChoice',\n",
       " 'FlaxAlbertForPreTraining',\n",
       " 'FlaxAlbertForQuestionAnswering',\n",
       " 'FlaxAlbertForSequenceClassification',\n",
       " 'FlaxAlbertForTokenClassification',\n",
       " 'FlaxAlbertModel',\n",
       " 'FlaxAlbertPreTrainedModel',\n",
       " 'FlaxAutoModel',\n",
       " 'FlaxAutoModelForCausalLM',\n",
       " 'FlaxAutoModelForImageClassification',\n",
       " 'FlaxAutoModelForMaskedLM',\n",
       " 'FlaxAutoModelForMultipleChoice',\n",
       " 'FlaxAutoModelForNextSentencePrediction',\n",
       " 'FlaxAutoModelForPreTraining',\n",
       " 'FlaxAutoModelForQuestionAnswering',\n",
       " 'FlaxAutoModelForSeq2SeqLM',\n",
       " 'FlaxAutoModelForSequenceClassification',\n",
       " 'FlaxAutoModelForSpeechSeq2Seq',\n",
       " 'FlaxAutoModelForTokenClassification',\n",
       " 'FlaxAutoModelForVision2Seq',\n",
       " 'FlaxBartDecoderPreTrainedModel',\n",
       " 'FlaxBartForCausalLM',\n",
       " 'FlaxBartForConditionalGeneration',\n",
       " 'FlaxBartForQuestionAnswering',\n",
       " 'FlaxBartForSequenceClassification',\n",
       " 'FlaxBartModel',\n",
       " 'FlaxBartPreTrainedModel',\n",
       " 'FlaxBeitForImageClassification',\n",
       " 'FlaxBeitForMaskedImageModeling',\n",
       " 'FlaxBeitModel',\n",
       " 'FlaxBeitPreTrainedModel',\n",
       " 'FlaxBertForCausalLM',\n",
       " 'FlaxBertForMaskedLM',\n",
       " 'FlaxBertForMultipleChoice',\n",
       " 'FlaxBertForNextSentencePrediction',\n",
       " 'FlaxBertForPreTraining',\n",
       " 'FlaxBertForQuestionAnswering',\n",
       " 'FlaxBertForSequenceClassification',\n",
       " 'FlaxBertForTokenClassification',\n",
       " 'FlaxBertModel',\n",
       " 'FlaxBertPreTrainedModel',\n",
       " 'FlaxBigBirdForCausalLM',\n",
       " 'FlaxBigBirdForMaskedLM',\n",
       " 'FlaxBigBirdForMultipleChoice',\n",
       " 'FlaxBigBirdForPreTraining',\n",
       " 'FlaxBigBirdForQuestionAnswering',\n",
       " 'FlaxBigBirdForSequenceClassification',\n",
       " 'FlaxBigBirdForTokenClassification',\n",
       " 'FlaxBigBirdModel',\n",
       " 'FlaxBigBirdPreTrainedModel',\n",
       " 'FlaxBlenderbotForConditionalGeneration',\n",
       " 'FlaxBlenderbotModel',\n",
       " 'FlaxBlenderbotPreTrainedModel',\n",
       " 'FlaxBlenderbotSmallForConditionalGeneration',\n",
       " 'FlaxBlenderbotSmallModel',\n",
       " 'FlaxBlenderbotSmallPreTrainedModel',\n",
       " 'FlaxCLIPModel',\n",
       " 'FlaxCLIPPreTrainedModel',\n",
       " 'FlaxCLIPTextModel',\n",
       " 'FlaxCLIPTextPreTrainedModel',\n",
       " 'FlaxCLIPVisionModel',\n",
       " 'FlaxCLIPVisionPreTrainedModel',\n",
       " 'FlaxDistilBertForMaskedLM',\n",
       " 'FlaxDistilBertForMultipleChoice',\n",
       " 'FlaxDistilBertForQuestionAnswering',\n",
       " 'FlaxDistilBertForSequenceClassification',\n",
       " 'FlaxDistilBertForTokenClassification',\n",
       " 'FlaxDistilBertModel',\n",
       " 'FlaxDistilBertPreTrainedModel',\n",
       " 'FlaxElectraForCausalLM',\n",
       " 'FlaxElectraForMaskedLM',\n",
       " 'FlaxElectraForMultipleChoice',\n",
       " 'FlaxElectraForPreTraining',\n",
       " 'FlaxElectraForQuestionAnswering',\n",
       " 'FlaxElectraForSequenceClassification',\n",
       " 'FlaxElectraForTokenClassification',\n",
       " 'FlaxElectraModel',\n",
       " 'FlaxElectraPreTrainedModel',\n",
       " 'FlaxEncoderDecoderModel',\n",
       " 'FlaxForcedBOSTokenLogitsProcessor',\n",
       " 'FlaxForcedEOSTokenLogitsProcessor',\n",
       " 'FlaxGPT2LMHeadModel',\n",
       " 'FlaxGPT2Model',\n",
       " 'FlaxGPT2PreTrainedModel',\n",
       " 'FlaxGPTJForCausalLM',\n",
       " 'FlaxGPTJModel',\n",
       " 'FlaxGPTJPreTrainedModel',\n",
       " 'FlaxGPTNeoForCausalLM',\n",
       " 'FlaxGPTNeoModel',\n",
       " 'FlaxGPTNeoPreTrainedModel',\n",
       " 'FlaxGenerationMixin',\n",
       " 'FlaxLogitsProcessor',\n",
       " 'FlaxLogitsProcessorList',\n",
       " 'FlaxLogitsWarper',\n",
       " 'FlaxLongT5ForConditionalGeneration',\n",
       " 'FlaxLongT5Model',\n",
       " 'FlaxLongT5PreTrainedModel',\n",
       " 'FlaxMBartForConditionalGeneration',\n",
       " 'FlaxMBartForQuestionAnswering',\n",
       " 'FlaxMBartForSequenceClassification',\n",
       " 'FlaxMBartModel',\n",
       " 'FlaxMBartPreTrainedModel',\n",
       " 'FlaxMT5EncoderModel',\n",
       " 'FlaxMT5ForConditionalGeneration',\n",
       " 'FlaxMT5Model',\n",
       " 'FlaxMarianMTModel',\n",
       " 'FlaxMarianModel',\n",
       " 'FlaxMarianPreTrainedModel',\n",
       " 'FlaxMinLengthLogitsProcessor',\n",
       " 'FlaxOPTForCausalLM',\n",
       " 'FlaxOPTModel',\n",
       " 'FlaxOPTPreTrainedModel',\n",
       " 'FlaxPegasusForConditionalGeneration',\n",
       " 'FlaxPegasusModel',\n",
       " 'FlaxPegasusPreTrainedModel',\n",
       " 'FlaxPreTrainedModel',\n",
       " 'FlaxRegNetForImageClassification',\n",
       " 'FlaxRegNetModel',\n",
       " 'FlaxRegNetPreTrainedModel',\n",
       " 'FlaxResNetForImageClassification',\n",
       " 'FlaxResNetModel',\n",
       " 'FlaxResNetPreTrainedModel',\n",
       " 'FlaxRoFormerForMaskedLM',\n",
       " 'FlaxRoFormerForMultipleChoice',\n",
       " 'FlaxRoFormerForQuestionAnswering',\n",
       " 'FlaxRoFormerForSequenceClassification',\n",
       " 'FlaxRoFormerForTokenClassification',\n",
       " 'FlaxRoFormerModel',\n",
       " 'FlaxRoFormerPreTrainedModel',\n",
       " 'FlaxRobertaForCausalLM',\n",
       " 'FlaxRobertaForMaskedLM',\n",
       " 'FlaxRobertaForMultipleChoice',\n",
       " 'FlaxRobertaForQuestionAnswering',\n",
       " 'FlaxRobertaForSequenceClassification',\n",
       " 'FlaxRobertaForTokenClassification',\n",
       " 'FlaxRobertaModel',\n",
       " 'FlaxRobertaPreLayerNormForCausalLM',\n",
       " 'FlaxRobertaPreLayerNormForMaskedLM',\n",
       " 'FlaxRobertaPreLayerNormForMultipleChoice',\n",
       " 'FlaxRobertaPreLayerNormForQuestionAnswering',\n",
       " 'FlaxRobertaPreLayerNormForSequenceClassification',\n",
       " 'FlaxRobertaPreLayerNormForTokenClassification',\n",
       " 'FlaxRobertaPreLayerNormModel',\n",
       " 'FlaxRobertaPreLayerNormPreTrainedModel',\n",
       " 'FlaxRobertaPreTrainedModel',\n",
       " 'FlaxSpeechEncoderDecoderModel',\n",
       " 'FlaxT5EncoderModel',\n",
       " 'FlaxT5ForConditionalGeneration',\n",
       " 'FlaxT5Model',\n",
       " 'FlaxT5PreTrainedModel',\n",
       " 'FlaxTemperatureLogitsWarper',\n",
       " 'FlaxTopKLogitsWarper',\n",
       " 'FlaxTopPLogitsWarper',\n",
       " 'FlaxViTForImageClassification',\n",
       " 'FlaxViTModel',\n",
       " 'FlaxViTPreTrainedModel',\n",
       " 'FlaxVisionEncoderDecoderModel',\n",
       " 'FlaxVisionTextDualEncoderModel',\n",
       " 'FlaxWav2Vec2ForCTC',\n",
       " 'FlaxWav2Vec2ForPreTraining',\n",
       " 'FlaxWav2Vec2Model',\n",
       " 'FlaxWav2Vec2PreTrainedModel',\n",
       " 'FlaxWhisperForAudioClassification',\n",
       " 'FlaxWhisperForConditionalGeneration',\n",
       " 'FlaxWhisperModel',\n",
       " 'FlaxWhisperPreTrainedModel',\n",
       " 'FlaxXGLMForCausalLM',\n",
       " 'FlaxXGLMModel',\n",
       " 'FlaxXGLMPreTrainedModel',\n",
       " 'FlaxXLMRobertaForCausalLM',\n",
       " 'FlaxXLMRobertaForMaskedLM',\n",
       " 'FlaxXLMRobertaForMultipleChoice',\n",
       " 'FlaxXLMRobertaForQuestionAnswering',\n",
       " 'FlaxXLMRobertaForSequenceClassification',\n",
       " 'FlaxXLMRobertaForTokenClassification',\n",
       " 'FlaxXLMRobertaModel',\n",
       " 'FlaxXLMRobertaPreTrainedModel',\n",
       " 'FocalNetBackbone',\n",
       " 'FocalNetConfig',\n",
       " 'FocalNetForImageClassification',\n",
       " 'FocalNetForMaskedImageModeling',\n",
       " 'FocalNetModel',\n",
       " 'FocalNetPreTrainedModel',\n",
       " 'ForcedBOSTokenLogitsProcessor',\n",
       " 'ForcedEOSTokenLogitsProcessor',\n",
       " 'FunnelBaseModel',\n",
       " 'FunnelConfig',\n",
       " 'FunnelForMaskedLM',\n",
       " 'FunnelForMultipleChoice',\n",
       " 'FunnelForPreTraining',\n",
       " 'FunnelForQuestionAnswering',\n",
       " 'FunnelForSequenceClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'FunnelModel',\n",
       " 'FunnelPreTrainedModel',\n",
       " 'FunnelTokenizer',\n",
       " 'FunnelTokenizerFast',\n",
       " 'GIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GLPNConfig',\n",
       " 'GLPNFeatureExtractor',\n",
       " 'GLPNForDepthEstimation',\n",
       " 'GLPNImageProcessor',\n",
       " 'GLPNModel',\n",
       " 'GLPNPreTrainedModel',\n",
       " 'GLPN_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GLPN_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPT2Config',\n",
       " 'GPT2DoubleHeadsModel',\n",
       " 'GPT2ForQuestionAnswering',\n",
       " 'GPT2ForSequenceClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'GPT2LMHeadModel',\n",
       " 'GPT2Model',\n",
       " 'GPT2PreTrainedModel',\n",
       " 'GPT2Tokenizer',\n",
       " 'GPT2TokenizerFast',\n",
       " 'GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPT2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPTBigCodeConfig',\n",
       " 'GPTBigCodeForCausalLM',\n",
       " 'GPTBigCodeForSequenceClassification',\n",
       " 'GPTBigCodeForTokenClassification',\n",
       " 'GPTBigCodeModel',\n",
       " 'GPTBigCodePreTrainedModel',\n",
       " 'GPTJConfig',\n",
       " 'GPTJForCausalLM',\n",
       " 'GPTJForQuestionAnswering',\n",
       " 'GPTJForSequenceClassification',\n",
       " 'GPTJModel',\n",
       " 'GPTJPreTrainedModel',\n",
       " 'GPTJ_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPTJ_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPTNeoConfig',\n",
       " 'GPTNeoForCausalLM',\n",
       " 'GPTNeoForQuestionAnswering',\n",
       " 'GPTNeoForSequenceClassification',\n",
       " 'GPTNeoForTokenClassification',\n",
       " 'GPTNeoModel',\n",
       " 'GPTNeoPreTrainedModel',\n",
       " 'GPTNeoXConfig',\n",
       " 'GPTNeoXForCausalLM',\n",
       " 'GPTNeoXForQuestionAnswering',\n",
       " 'GPTNeoXForSequenceClassification',\n",
       " 'GPTNeoXForTokenClassification',\n",
       " 'GPTNeoXJapaneseConfig',\n",
       " 'GPTNeoXJapaneseForCausalLM',\n",
       " 'GPTNeoXJapaneseLayer',\n",
       " 'GPTNeoXJapaneseModel',\n",
       " 'GPTNeoXJapanesePreTrainedModel',\n",
       " 'GPTNeoXJapaneseTokenizer',\n",
       " 'GPTNeoXLayer',\n",
       " 'GPTNeoXModel',\n",
       " 'GPTNeoXPreTrainedModel',\n",
       " 'GPTNeoXTokenizerFast',\n",
       " 'GPTSAN_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPTSAN_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPTSanJapaneseConfig',\n",
       " 'GPTSanJapaneseForConditionalGeneration',\n",
       " 'GPTSanJapaneseModel',\n",
       " 'GPTSanJapanesePreTrainedModel',\n",
       " 'GPTSanJapaneseTokenizer',\n",
       " 'GPTSw3Tokenizer',\n",
       " 'GPT_BIGCODE_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPT_BIGCODE_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPT_NEOX_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPT_NEOX_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPT_NEOX_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GPT_NEO_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPT_NEO_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GRAPHORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GRAPHORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GenerationConfig',\n",
       " 'GenerationMixin',\n",
       " 'GitConfig',\n",
       " 'GitForCausalLM',\n",
       " 'GitModel',\n",
       " 'GitPreTrainedModel',\n",
       " 'GitProcessor',\n",
       " 'GitVisionConfig',\n",
       " 'GitVisionModel',\n",
       " 'GlueDataTrainingArguments',\n",
       " 'GlueDataset',\n",
       " 'GradientAccumulator',\n",
       " 'GraphormerConfig',\n",
       " 'GraphormerForGraphClassification',\n",
       " 'GraphormerModel',\n",
       " 'GraphormerPreTrainedModel',\n",
       " 'GroupViTConfig',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bert.embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n\u001b[1;32m      6\u001b[0m \u001b[39m# Extract the embedding layer\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embedding_layer \u001b[39m=\u001b[39m state_dict[\u001b[39m\"\u001b[39;49m\u001b[39mbert.embeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      9\u001b[0m \u001b[39m# Extract the first transformer layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m transformer_layer_1 \u001b[39m=\u001b[39m state_dict[\u001b[39m\"\u001b[39m\u001b[39mbert.encoder.layer.0\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bert.embeddings'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract the embedding layer\n",
    "embedding_layer = state_dict[\"bert.embeddings\"]\n",
    "\n",
    "# Extract the first transformer layer\n",
    "transformer_layer_1 = state_dict[\"bert.encoder.layer.0\"]\n",
    "\n",
    "# Extract the second attention head in the second transformer layer\n",
    "attention_head_2 = state_dict[\"bert.encoder.layer.1.attention.self.query.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.position_ids', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['embeddings.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['embeddings.position_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91c69d281b806a3add6b161e23d9089dcb392788fdbe6cdd17c006940cab5b65"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
