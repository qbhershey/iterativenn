{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fb26da",
   "metadata": {},
   "source": [
    "# Load libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6c0ee4",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/iterativenn/notebooks/START_HERE/0-rcpMLP-vs-INN.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Blegendary-barnacle-q7v9gx4jjr9fwwq/workspaces/iterativenn/notebooks/START_HERE/0-rcpMLP-vs-INN.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Blegendary-barnacle-q7v9gx4jjr9fwwq/workspaces/iterativenn/notebooks/START_HERE/0-rcpMLP-vs-INN.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_factory\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelFactory_callback\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Blegendary-barnacle-q7v9gx4jjr9fwwq/workspaces/iterativenn/notebooks/START_HERE/0-rcpMLP-vs-INN.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_factory\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFactory\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Blegendary-barnacle-q7v9gx4jjr9fwwq/workspaces/iterativenn/notebooks/START_HERE/0-rcpMLP-vs-INN.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger_factory\u001b[39;00m \u001b[39mimport\u001b[39;00m LoggerFactory\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Blegendary-barnacle-q7v9gx4jjr9fwwq/workspaces/iterativenn/notebooks/START_HERE/0-rcpMLP-vs-INN.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlit_modules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mIteratedModel\u001b[39;00m \u001b[39mimport\u001b[39;00m IteratedModel, ConfigCallbacks\n",
      "File \u001b[0;32m/workspaces/iterativenn/src/iterativenn/utils/data_factory.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhydra\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m instantiate\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mDatasetUtils\u001b[39;00m \u001b[39mimport\u001b[39;00m AdditionImageSequence, GymImageSequence, ImageSequence, RandomImageSequence\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mDatasetUtils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_transform\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterativenn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperations\u001b[39;00m \u001b[39mimport\u001b[39;00m trivial_collate_fn\n",
      "File \u001b[0;32m/workspaces/iterativenn/src/iterativenn/utils/DatasetUtils.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m     10\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m) \n\u001b[0;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "from iterativenn.utils.model_factory import ModelFactory_callback\n",
    "from iterativenn.utils.data_factory import DataFactory\n",
    "from iterativenn.utils.logger_factory import LoggerFactory\n",
    "from iterativenn.lit_modules.IteratedModel import IteratedModel, ConfigCallbacks\n",
    "from iterativenn.nn_modules.Sequential1D import Sequential1D\n",
    "from iterativenn.nn_modules.MaskedLinear import MaskedLinear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729cfdd",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc17b4",
   "metadata": {},
   "source": [
    "This notebook is intended to be, in as simple a way as possible, a comparison of a classic MLP with an INN. \n",
    "\n",
    "1.  This will be a baseline solution that should be familiar to most students.\n",
    "2.  It will be a sanity check on the MLP and INN solutions.  I.e., they should be able to learn the same thing as the least squares solution, if not better.\n",
    "3.  It will be a starting place for any student working on similar problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c91ff0",
   "metadata": {},
   "source": [
    "# Top level parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91320c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals = {}\n",
    "globals['layer_sizes'] = [2500, 500, 200, 10]\n",
    "globals['activation'] = 'ELU'\n",
    "globals['accelerator'] = 'auto'\n",
    "# globals['accelerator'] = 'cpu'\n",
    "globals['optimizer'] = 'Adam'\n",
    "globals['learning rate'] = 0.01\n",
    "globals['bias'] = False\n",
    "globals['initialization mean'] = 0.0\n",
    "globals['initialization std'] = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals['do_MLP'] = True\n",
    "globals['do_sequence_MLP'] = True\n",
    "globals['do_INN'] = True\n",
    "globals['do_sparse_INN'] = True\n",
    "globals['saved models'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e519f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_dev_run = False\n",
    "if fast_dev_run:\n",
    "    globals['number_of_runs'] = 1\n",
    "    globals['data size'] = 16\n",
    "    globals['batch size'] = 8\n",
    "    globals['epochs'] = 1\n",
    "else:\n",
    "    globals['number_of_runs'] = 3\n",
    "    globals['data size'] = 1000\n",
    "    globals['batch size'] = 128\n",
    "    globals['epochs'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c6670",
   "metadata": {},
   "source": [
    "## MLP data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc196",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4f35c",
   "metadata": {},
   "source": [
    "Here we begin by loading the data.  We will ust the classic MNIST data set for these examples.  The iterativenn package provides utilities to make reading in the data easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_data_loader():\n",
    "    data_cfg = {\n",
    "        'dataset': 'MNIST',\n",
    "        'transform': 'both',\n",
    "        'train_size': globals['data size'],\n",
    "        'validation_size': globals['data size'],\n",
    "        'test_size': globals['data size'],\n",
    "        'batch_size': globals['batch size'],\n",
    "    }\n",
    "\n",
    "    data = DataFactory(data_cfg)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292eecd5",
   "metadata": {},
   "source": [
    "Each item is a tuple with two entries. The first is an image and the second is a label.  \n",
    "\n",
    "The image is a 50x50 array of pixels.  Note, this is different from the MNIST standard of 28x28!   We scale the images up so that we can do\n",
    "transformations on them to make the problem more interesting.\n",
    "\n",
    "The label is a number between 0 and 9.  The label is the number that the image represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b07afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_MLP']:\n",
    "    data = MLP_data_loader()\n",
    "    sample = next(iter(data.train_dataloader()))\n",
    "    for i in range(3):\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(sample[0][i, 0, :, :])\n",
    "        plt.title(f'true category {sample[1][i]}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8bd3a",
   "metadata": {},
   "source": [
    "## MLP sequence data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_sequence_data_loader():\n",
    "    data_cfg = {\n",
    "        'dataset': 'MNIST',\n",
    "        'transform': 'both',\n",
    "        'train_size': globals['data size'],\n",
    "        'validation_size': globals['data size'],\n",
    "        'test_size': globals['data size'],\n",
    "        'batch_size': globals['batch size'],\n",
    "        'sequence_dict': {\n",
    "            'type': 'uniform',\n",
    "            'min_copies': 1,\n",
    "            'max_copies': 1,\n",
    "            'evaluate_loss': 'last'\n",
    "        }\n",
    "    }\n",
    "    data = DataFactory(data_cfg)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c229e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_sequence_MLP']:\n",
    "    data = MLP_sequence_data_loader()\n",
    "    sample = next(iter(data.train_dataloader()))\n",
    "    sample[0]['x']\n",
    "    for i in range(3):\n",
    "        copies = len(sample[i]['x'])\n",
    "        _, ax = plt.subplots(ncols=copies, squeeze=False)\n",
    "        for j in range(copies):\n",
    "            ax[0, j].imshow(sample[i]['x'][j][0, :, :])\n",
    "            ax[0, j].set_title(f'{sample[i][\"y\"][j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394f966",
   "metadata": {},
   "source": [
    "## INN data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4303bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def INN_data_loader():\n",
    "    data_cfg = {\n",
    "        'dataset': 'MNIST',\n",
    "        'transform': 'both',\n",
    "        'train_size': globals['data size'],\n",
    "        'validation_size': globals['data size'],\n",
    "        'test_size': globals['data size'],\n",
    "        'batch_size': globals['batch size'],\n",
    "        'sequence_dict': {\n",
    "            'type': 'uniform',\n",
    "            'min_copies': 3,\n",
    "            'max_copies': 3,\n",
    "            'evaluate_loss': 'last'\n",
    "        }\n",
    "    }\n",
    "    data = DataFactory(data_cfg)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_INN']:\n",
    "    data = INN_data_loader()\n",
    "    sample = next(iter(data.train_dataloader()))\n",
    "    sample[0]['x']\n",
    "    for i in range(3):\n",
    "        copies = len(sample[i]['x'])\n",
    "        _, ax = plt.subplots(ncols=copies)\n",
    "        for j in range(copies):\n",
    "            ax[j].imshow(sample[i]['x'][j][0, :, :])\n",
    "            ax[j].set_title(f'{sample[i][\"y\"][j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca930c0",
   "metadata": {},
   "source": [
    "# MLP solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.losses = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.module(X)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X,Y = batch\n",
    "        Y_hat = self.module(X.view(X.shape[0], -1))\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction='sum')(Y_hat, Y)\n",
    "        self.losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if globals['optimizer'] == 'Adam':\n",
    "            optimizer = torch.optim.Adam(params=self.parameters(), lr=globals['learning rate'])\n",
    "        elif globals['optimizer'] == 'SGD':\n",
    "            return torch.optim.SGD(params=self.parameters(), lr=globals['learning rate'])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP():\n",
    "    data = MLP_data_loader()\n",
    "    # Sequential1D is just a wrapper around nn.Sequential that\n",
    "    # records the input and output sizes of the network.  This is useful\n",
    "    # for the INN solution below as the MLP can be used as a block in an INN.\n",
    "    # However, the as far as this part is concerned, it is identical to nn.Sequential.\n",
    "    MLP = Sequential1D(in_features=globals['layer_sizes'][0],\n",
    "                       out_features=globals['layer_sizes'][-1]) \n",
    "\n",
    "    for i in range(len(globals['layer_sizes'])-1):\n",
    "        linear_module = torch.nn.Linear(globals['layer_sizes'][i], globals['layer_sizes'][i+1], bias=globals['bias'])\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.normal_(linear_module.weight, \n",
    "                                  globals['initialization mean'], \n",
    "                                  globals['initialization std'])\n",
    "        MLP.add_module(f'linear{i}', linear_module)\n",
    "        # always end with a linear layer\n",
    "        if i < len(globals['layer_sizes'])-2:      \n",
    "            if globals['activation'] == 'ELU':\n",
    "                MLP.add_module(f'activation{i}', torch.nn.ELU())      \n",
    "            else:\n",
    "                raise ValueError(f'Unknown activation {globals[\"activation\"]}')\n",
    "\n",
    "    MLP_module = LitModule(MLP)\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=globals['epochs'],\n",
    "                         logger=LoggerFactory(),\n",
    "                        devices=1 if torch.cuda.is_available() else 1,\n",
    "                        accelerator=globals['accelerator'])\n",
    "    trainer.fit(MLP_module, data)\n",
    "    return MLP_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecfe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_MLP']:\n",
    "    name = 'MLP'\n",
    "    globals['saved models'][name] = {}\n",
    "    # run all the models and save them\n",
    "    globals['saved models'][name]['models'] = [train_MLP() for i in range(globals['number_of_runs'])]\n",
    "    # save the number of trainable parameters\n",
    "    globals['saved models'][name]['average trainable params'] = np.mean([sum(p.numel() for p in model.parameters() if p.requires_grad) for model in globals['saved models'][name]['models']])\n",
    "    # save the losses\n",
    "    losses = [torch.tensor(model.losses).detach().numpy() for model in globals['saved models'][name]['models']]\n",
    "    globals['saved models'][name]['average losses'] = np.mean(losses, axis=0)\n",
    "    globals['saved models'][name]['std losses'] = np.std(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_MLP']:\n",
    "    name = 'MLP'\n",
    "    plt.errorbar(range(len(globals['saved models'][name]['average losses'])), \n",
    "                 globals['saved models'][name]['average losses'], \n",
    "                 globals['saved models'][name]['std losses'],\n",
    "                 capsize=5,\n",
    "                 label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d541f3d",
   "metadata": {},
   "source": [
    "# MLP using sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab07e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequence_MLP():\n",
    "    data = MLP_sequence_data_loader()\n",
    "    # Sequential1D is just a wrapper around nn.Sequential that\n",
    "    # records the input and output sizes of the network.  This is useful\n",
    "    # for the INN solution below as the MLP can be used as a block in an INN.\n",
    "    # However, the as far as this part is concerned, it is identical to nn.Sequential.\n",
    "    MLP = Sequential1D(in_features=globals['layer_sizes'][0],\n",
    "                       out_features=globals['layer_sizes'][-1]) \n",
    "\n",
    "    for i in range(len(globals['layer_sizes'])-1):\n",
    "\n",
    "        # You can use a masked linear layer if you want\n",
    "        linear_module = MaskedLinear(globals['layer_sizes'][i], \n",
    "                                        globals['layer_sizes'][i+1], \n",
    "                                        bias=globals['bias'])\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.normal_(linear_module.weight_0, \n",
    "                                  globals['initialization mean'], \n",
    "                                  globals['initialization std'])\n",
    "\n",
    "        # Or you can use a regular linear layer, the answers are the same.\n",
    "        # linear_module = torch.nn.Linear(globals['layer_sizes'][i], \n",
    "        #                                 globals['layer_sizes'][i+1], \n",
    "        #                                 bias=globals['bias'])\n",
    "        # with torch.no_grad():\n",
    "        #     torch.nn.init.normal_(linear_module.weight, \n",
    "        #                           globals['initialization mean'], \n",
    "        #                           globals['initialization std'])\n",
    "\n",
    "        MLP.add_module(f'linear{i}', linear_module)\n",
    "        # always end with a linear layer\n",
    "        if i < len(globals['layer_sizes'])-2:      \n",
    "            if globals['activation'] == 'ELU':\n",
    "                MLP.add_module(f'activation{i}', torch.nn.ELU())      \n",
    "            else:\n",
    "                raise ValueError(f'Unknown activation {globals[\"activation\"]}')\n",
    "\n",
    "    cfg = {\n",
    "        \"callbacks\": {\n",
    "            \"loss\": {\n",
    "                \"func\": \"CrossEntropyLoss\",\n",
    "                \"idx_list\" : range(0, globals['layer_sizes'][-1]),\n",
    "            },\n",
    "            \"initialization\": {\n",
    "                \"func\": \"zeros\",\n",
    "                \"size\": globals['layer_sizes'][0],\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"func\": \"insert\",\n",
    "                \"idx_list\": range(0, globals['layer_sizes'][0]),\n",
    "                \"flatten_input\": True,\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"func\": \"max\",\n",
    "                \"idx_list\" : range(0, globals['layer_sizes'][-1]),\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    MLP_callbacks = ConfigCallbacks(cfg['callbacks'])\n",
    "\n",
    "    MLP_module = IteratedModel(MLP, MLP_callbacks, normalize_loss=False, \n",
    "                              learning_rate=globals['learning rate'], optimizer=globals['optimizer'])\n",
    "    trainer = pl.Trainer(max_epochs=globals['epochs'],\n",
    "                         devices=1 if torch.cuda.is_available() else 1,\n",
    "                         logger=LoggerFactory(),\n",
    "                         accelerator=globals['accelerator'])\n",
    "    trainer.fit(MLP_module, data)\n",
    "    return MLP_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c80f2",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "if globals['do_sequence_MLP']:\n",
    "    name = 'sequence_MLP'\n",
    "    globals['saved models'][name] = {}\n",
    "    # run all the models and save them\n",
    "    globals['saved models'][name]['models'] = [train_sequence_MLP() for i in range(globals['number_of_runs'])]\n",
    "    # save the number of trainable parameters\n",
    "    globals['saved models'][name]['average trainable params'] = np.mean([model.number_of_trainable_parameters() for model in globals['saved models'][name]['models']])\n",
    "    # save the losses\n",
    "    losses = [torch.tensor(model.losses).detach().numpy() for model in globals['saved models'][name]['models']]\n",
    "    globals['saved models'][name]['average losses'] = np.mean(losses, axis=0)\n",
    "    globals['saved models'][name]['std losses'] = np.std(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dafab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_sequence_MLP']:\n",
    "    name = 'sequence_MLP'\n",
    "    plt.errorbar(range(len(globals['saved models'][name]['average losses'])), \n",
    "                 globals['saved models'][name]['average losses'], \n",
    "                 globals['saved models'][name]['std losses'],\n",
    "                 capsize=5,\n",
    "                 label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd573f",
   "metadata": {},
   "source": [
    "# INN that simulate MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbdadd",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{bmatrix}\n",
    "x & 0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I & W_1 & 0   &  0 \\\\\n",
    "0 & 0   & W_2 &  0 \\\\\n",
    "0 & 0   & 0   & W_3 \\\\\n",
    "0 & 0   & 0   & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x & W_1 x & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f3fe1",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{bmatrix}\n",
    "x & W_1 x & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I & W_1 & 0   &  0 \\\\\n",
    "0 & 0   & W_2 &  0 \\\\\n",
    "0 & 0   & 0   & W_3 \\\\\n",
    "0 & 0   & 0   & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x & W_1 x & W_2 W_1 x & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9c116",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{bmatrix}\n",
    "x & W_1 x & W_2 W_1 x & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I & W_1 & 0   &  0 \\\\\n",
    "0 & 0   & W_2 &  0 \\\\\n",
    "0 & 0   & 0   & W_3 \\\\\n",
    "0 & 0   & 0   & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x & W_1 x & W_2 W_1 x & W_3 W_2 W_1 x\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_INN():\n",
    "    data = INN_data_loader()\n",
    "    default_module = 'MaskedLinear.from_description'\n",
    "    default_module_kwargs = {'block_type': 'W', \n",
    "                             'initialization_type': f'G={globals[\"initialization mean\"]},{globals[\"initialization std\"]}', \n",
    "                             'trainable': True, 'bias': globals['bias']}\n",
    "    \n",
    "    cfg = {\n",
    "        \"model_type\": \"sequential2D\",   \n",
    "        \"in_features_list\": globals['layer_sizes'], \n",
    "        \"out_features_list\": globals['layer_sizes'], \n",
    "\n",
    "        \"block_types\": [\n",
    "            ['Identity', default_module, None,           None],\n",
    "            [None,       None,           default_module, None],\n",
    "            [None,       None,           None,           default_module],\n",
    "            [None,       None,           None,          None],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [None, default_module_kwargs, None, None],\n",
    "            [None, None, default_module_kwargs, None],\n",
    "            [None, None, None, default_module_kwargs],\n",
    "            [None, None, None, None],\n",
    "        ],\n",
    "\n",
    "        \"activations\" : ['Identity', 'ELU', 'ELU', 'Identity'],\n",
    "        \"activation_sizes\" : globals['layer_sizes'],\n",
    "        \"callbacks\": {\n",
    "            \"loss\": {\n",
    "                \"func\": \"CrossEntropyLoss\",\n",
    "                \"idx_list\" : range(sum(globals['layer_sizes'][:-1]), sum(globals['layer_sizes'])),\n",
    "            },\n",
    "            \"initialization\": {\n",
    "                \"func\": \"zeros\",\n",
    "                \"size\": sum(globals['layer_sizes']),\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"func\": \"insert\",\n",
    "                \"idx_list\": range(0, globals['layer_sizes'][0]),\n",
    "                \"flatten_input\": True,\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"func\": \"max\",\n",
    "                \"idx_list\" : range(sum(globals['layer_sizes'][:-1]), sum(globals['layer_sizes']))\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    # This is an IteratedModule, which is a wrapper around a module\n",
    "    module = ModelFactory_callback(cfg)\n",
    "    INN_module = IteratedModel(module.model, module.callbacks, normalize_loss=False, \n",
    "                            learning_rate=globals['learning rate'], optimizer=globals['optimizer'])\n",
    "    trainer = pl.Trainer(max_epochs=globals['epochs'],\n",
    "                         logger=LoggerFactory(),\n",
    "                         devices=1 if torch.cuda.is_available() else 1,\n",
    "                         accelerator=globals['accelerator'])\n",
    "    trainer.fit(INN_module, data)\n",
    "    return INN_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedc2e0",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "if globals['do_INN']:\n",
    "    name = 'INN'\n",
    "    globals['saved models'][name] = {}\n",
    "    # run all the models and save them\n",
    "    globals['saved models'][name]['models'] = [train_INN() for i in range(globals['number_of_runs'])]\n",
    "    # save the number of trainable parameters\n",
    "    globals['saved models'][name]['average trainable params'] = np.mean([model.number_of_trainable_parameters() for model in globals['saved models'][name]['models']])\n",
    "    # save the losses\n",
    "    losses = [torch.tensor(model.losses).detach().numpy() for model in globals['saved models'][name]['models']]\n",
    "    globals['saved models'][name]['average losses'] = np.mean(losses, axis=0)\n",
    "    globals['saved models'][name]['std losses'] = np.std(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_INN']:\n",
    "    name = 'INN'\n",
    "    _, ax = plt.subplots(nrows=3, figsize=(10, 20))\n",
    "\n",
    "    ax[0].errorbar(range(len(globals['saved models'][name]['average losses'])), \n",
    "                   globals['saved models'][name]['average losses'], \n",
    "                   globals['saved models'][name]['std losses'],\n",
    "                   capsize=5,\n",
    "                   label=name)\n",
    "    \n",
    "    plot_model = globals['saved models'][name]['models'][0]\n",
    "    I = torch.zeros(plot_model.model[0].in_features, plot_model.model[0].in_features)\n",
    "    for i in range(plot_model.model[0].in_features):\n",
    "        I[i, i] = 1.0\n",
    "    M = plot_model.model[0].forward(I).detach().numpy()\n",
    "    ax[1].spy(M, markersize=0.01)\n",
    "    U,E,VT = np.linalg.svd(M)\n",
    "    ax[2].plot(E)\n",
    "    globals['saved models'][name]['singular values'] = E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d228cd",
   "metadata": {},
   "source": [
    "# Random INN example that is same size as MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c10cac",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{bmatrix}\n",
    "x & 0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I & S & S & S \\\\\n",
    "0 & S & S & S \\\\\n",
    "0 & S & S & S \\\\\n",
    "0 & S & S & S\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sparse_INN():    \n",
    "    data = INN_data_loader()\n",
    "    default_module = 'SparseLinear.from_singleBlock'\n",
    "    default_module_kwargs = {'block_type': 'R=0.4', 'initialization_type': f'G={globals[\"initialization mean\"]},{globals[\"initialization std\"]}', 'bias': globals[\"bias\"]}\n",
    "    # default_module = 'MaskedLinear.from_description'\n",
    "    # default_module = 'SparseLinear.from_description'\n",
    "    # default_module_kwargs = {'block_type': 'R=0.6', 'initialization_type': f'G={globals[\"initialization mean\"]},{globals[\"initialization std\"]}', 'trainable': 'non-zero', 'bias': globals[\"bias\"]}\n",
    "\n",
    "    cfg = {\n",
    "        \"model_type\": \"sequential2D\",   \n",
    "        \"in_features_list\": globals['layer_sizes'], \n",
    "        \"out_features_list\": globals['layer_sizes'], \n",
    "\n",
    "        \"block_types\": [\n",
    "            ['Identity', default_module, default_module, default_module],\n",
    "            [None,       default_module, default_module, default_module],\n",
    "            [None,       default_module, default_module, default_module],\n",
    "            [None,       default_module, default_module, default_module],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [None, default_module_kwargs, default_module_kwargs, default_module_kwargs],\n",
    "            [None, default_module_kwargs, default_module_kwargs, default_module_kwargs],\n",
    "            [None, default_module_kwargs, default_module_kwargs, default_module_kwargs],\n",
    "            [None, default_module_kwargs, default_module_kwargs, default_module_kwargs],\n",
    "        ],\n",
    "\n",
    "        \"activations\" : ['Identity', 'ELU', 'ELU', 'Identity'],\n",
    "        \"activation_sizes\" : globals['layer_sizes'],\n",
    "        \"callbacks\": {\n",
    "            \"loss\": {\n",
    "                \"func\": \"CrossEntropyLoss\",\n",
    "                \"idx_list\" : range(sum(globals['layer_sizes'][:-1]), sum(globals['layer_sizes'])),\n",
    "            },\n",
    "            \"initialization\": {\n",
    "                \"func\": \"zeros\",\n",
    "                \"size\": sum(globals['layer_sizes']),\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"func\": \"insert\",\n",
    "                \"idx_list\": range(0, globals['layer_sizes'][0]),\n",
    "                \"flatten_input\": True,\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"func\": \"max\",\n",
    "                \"idx_list\" : range(sum(globals['layer_sizes'][:-1]), sum(globals['layer_sizes']))\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    # This is an IteratedModule, which is a wrapper around a module\n",
    "    module = ModelFactory_callback(cfg)\n",
    "    INN_module = IteratedModel(module.model, module.callbacks, normalize_loss=False, \n",
    "                               learning_rate=globals['learning rate'], optimizer=globals['optimizer'])\n",
    "    trainer = pl.Trainer(max_epochs=globals['epochs'],\n",
    "                         logger=LoggerFactory(),\n",
    "                         devices=1 if torch.cuda.is_available() else 1,\n",
    "                         accelerator=globals['accelerator'])\n",
    "    trainer.fit(INN_module, data)\n",
    "    return INN_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_sparse_INN']:\n",
    "    name = 'sparse INN'\n",
    "    globals['saved models'][name] = {}\n",
    "    # run all the models and save them\n",
    "    globals['saved models'][name]['models'] = [train_sparse_INN() for i in range(globals['number_of_runs'])]\n",
    "    # save the number of trainable parameters\n",
    "    globals['saved models'][name]['average trainable params'] = np.mean([model.number_of_trainable_parameters() for model in globals['saved models'][name]['models']])\n",
    "    # save the losses\n",
    "    losses = [torch.tensor(model.losses).detach().numpy() for model in globals['saved models'][name]['models']]\n",
    "    globals['saved models'][name]['average losses'] = np.mean(losses, axis=0)\n",
    "    globals['saved models'][name]['std losses'] = np.std(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals['do_sparse_INN']:\n",
    "    name = 'sparse INN'\n",
    "    _, ax = plt.subplots(nrows=3, figsize=(10, 20))\n",
    "\n",
    "    ax[0].errorbar(range(len(globals['saved models'][name]['average losses'])), \n",
    "                   globals['saved models'][name]['average losses'], \n",
    "                   globals['saved models'][name]['std losses'],\n",
    "                   capsize=5,\n",
    "                   label=name)\n",
    "    \n",
    "    plot_model = globals['saved models'][name]['models'][0]\n",
    "    I = torch.zeros(plot_model.model[0].in_features, plot_model.model[0].in_features)\n",
    "    for i in range(plot_model.model[0].in_features):\n",
    "        I[i, i] = 1.0\n",
    "    M = plot_model.model[0].forward(I).detach().numpy()\n",
    "    ax[1].spy(M, markersize=0.01)\n",
    "    U,E,VT = np.linalg.svd(M)\n",
    "    ax[2].plot(E)\n",
    "    globals['saved models'][name]['singular values'] = E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162b09e",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1bd42",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "for name in globals['saved models']:\n",
    "    plt.errorbar(range(len(globals['saved models'][name]['average losses'])), \n",
    "                globals['saved models'][name]['average losses'], \n",
    "                globals['saved models'][name]['std losses'],\n",
    "                capsize=5,\n",
    "                label=name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in globals['saved models']:\n",
    "    total_parameters = globals['saved models'][name]['average trainable params']\n",
    "    average_loss = globals['saved models'][name]['average losses']\n",
    "    print(f'{name} has {total_parameters} trainable parameters and an average loss of {average_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3ab21",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for name in globals['saved models']:\n",
    "    if 'singular values' in globals['saved models'][name].keys():\n",
    "        plt.semilogy(globals['saved models'][name]['singular values'][:1000], label=name)\n",
    "        plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
