{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98db5a1-8385-4a5e-9447-503de5f56866",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7f4d4e-bd21-4e1c-be95-e58f1bcc743c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbd5834a9f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as py\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80f39a-a0be-4093-9b4e-39078771d110",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sparse matrix operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ddb36",
   "metadata": {},
   "source": [
    "$\\mathrm{Init}$ is initialization,\n",
    "$\\Omega$ is the mask,\n",
    "and $U$ is the update.  $U$ is the only thing that has a gradient\n",
    "\n",
    "The operator is\n",
    "\n",
    "$$\n",
    "S = \\mathrm{Init} + \\Omega \\odot U\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the Hadamard product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0253dc5-ed82-411b-b168-fc4eba0e6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMatrix(object):\n",
    "    def __init__(self, U=None, Init=None, Omega=None, Init_noise=0.1):\n",
    "        if U is None:\n",
    "            self.U = torch.zeros((3, 3), requires_grad=True)\n",
    "        else:\n",
    "            self.U = U\n",
    "\n",
    "        # Note, Init_noise == 0 is either a saddle or a maximum, so \n",
    "        # the gradient is 0 there and we can't optimize. We add a little\n",
    "        # noise to get away from that point\n",
    "        if Init is None:\n",
    "            self.Init = torch.rand((3, 3), requires_grad=False)*Init_noise\n",
    "        else:\n",
    "            self.Init = U\n",
    "\n",
    "        if Omega is None:\n",
    "            self.Omega = torch.tensor([[1, 1, 1],\n",
    "                                       [1, 1, 1],\n",
    "                                       [1, 1, 1]], requires_grad=False)\n",
    "        else:\n",
    "            self.Omega = Omega\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return (self.Init+self.Omega*self.U) @ X        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55daf6b4-73a7-41d0-874e-fbf5772a3270",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6dd6ae",
   "metadata": {},
   "source": [
    "We start with the 3-dimensional case.  So, \n",
    "\n",
    "$$\\mathrm{Init} + \\Omega \\odot U \\in \\mathbb{R}^{3 \\times 3}$$ \n",
    "\n",
    "so the data $X \\in \\mathbb{R}^{3 \\times n}$ where $n$ is the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c0411-8c00-4df4-b02d-f2dfed1391d1",
   "metadata": {},
   "source": [
    "The equation we want to solve is\n",
    "\n",
    "$$\n",
    "(\\mathrm{Init} + \\Omega \\odot U) X = Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c983c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.],\n",
    "                  [0.],\n",
    "                  [0.]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323c1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor([[0.],\n",
    "                  [0.],\n",
    "                  [2.]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fc48f-3a4c-45b5-ba6f-39029a3fe6e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An exact solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e024e20",
   "metadata": {},
   "source": [
    "We can compute the exact solution\n",
    "\n",
    "$ (\\mathrm{Init} + \\Omega \\odot U)X = Y $\n",
    "\n",
    "Assume $\\Omega = \\mathbf{1}$ so $\\Omega \\odot U = U$.\n",
    "\n",
    "$$ (\\mathrm{Init} + U)X = Y $$\n",
    "$$ \\mathrm{Init}X + UX = Y $$\n",
    "$$ UX = Y - \\mathrm{Init}X $$\n",
    "$$ U = (Y -\\mathrm{Init}X)X^{-1} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192a3bb-6d9f-475d-9d00-29e41be8f4d1",
   "metadata": {},
   "source": [
    "Now, the equation above needs to be interpreted carfully.  \n",
    "\n",
    "If $X \\in \\mathbb{R}^{3 \\times 3}$ the inverse requires that $X$ is full rank.\n",
    "\n",
    "If $X \\in \\mathbb{R}^{3 \\times k}$ with $k<3$ then there are multiple possible values for $U$, given by appending arbitrary (though full rank) columns to $X$.\n",
    "\n",
    "If $X \\in \\mathbb{R}^{3 \\times k}$ with $k>3$ then the mapping is only minimal in a least-squares and $X^{-1}$ is really a pseudo-inverse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d6df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should be the 0 vector\n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Make pad out X to make it square\n",
    "    X_padded = torch.hstack((X,torch.rand((3,2))))\n",
    "    # Make a sparse matrix\n",
    "    S = SparseMatrix(Init_noise=0.0)\n",
    "    # This is one possible U_exact\n",
    "    U_exact = (Y-S.Init @ X_padded) @ X_padded.inverse()\n",
    "    # Make the exact solution\n",
    "    S_exact = SparseMatrix(U=U_exact, Init_noise=0.0)\n",
    "    print('This should be the 0 vector\\n', S_exact(X) - Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324dffba-4425-4e2c-b10b-2c3c3604d989",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d32315",
   "metadata": {},
   "source": [
    "I am going to try to put the \"training data\" into the activation function. This needs more explanation, but I am not sure I understand it yet :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2cb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(X, iteration):\n",
    "    #X[1, :] = X[1, :]**3\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2846fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y, YHat, iteration):\n",
    "    # This is the Forbenius norm\n",
    "    return torch.linalg.matrix_norm(Y - YHat, ord='fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3079e0-ae58-4479-8199-c5f6151751a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put this outside so that we can run the loop many times.\n",
    "S = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "027fba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 1.9845165014266968\n",
      "epoch 40 loss 1.957511067390442\n",
      "epoch 80 loss 1.8826569318771362\n",
      "epoch 120 loss 1.6755435466766357\n",
      "epoch 160 loss 1.1391788721084595\n",
      "epoch 200 loss 0.15699271857738495\n",
      "epoch 206 final loss 0.002248641336336732\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD([S.U], lr=0.01)\n",
    "# The outer loop is the number of training epochs\n",
    "epochs = 401\n",
    "iterations = 2\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    interim_loss = 0\n",
    "    # The inner loop is the number of iterations\n",
    "    YHat = X.clone()\n",
    "    for k in range(iterations):\n",
    "        YHat = activation(S(YHat), k)\n",
    "        interim_loss += loss_function(Y, YHat, k)\n",
    "\n",
    "    final_loss = loss_function(Y, YHat, k)\n",
    "    \n",
    "    loss = 1*final_loss + 0*interim_loss\n",
    "\n",
    "    if i%(epochs//10)==0:\n",
    "        print(f'epoch {i} loss {loss}')\n",
    "\n",
    "    if loss > 0.01:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        print(f'epoch {i} final loss {loss}')\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b92b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8893e-04],\n",
       "        [ 2.1037e-03],\n",
       "        [ 1.9999e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.],\n",
    "                  [0.],\n",
    "                  [0.]], requires_grad=False)\n",
    "with torch.no_grad():\n",
    "    for k in range(iterations):\n",
    "        X = activation(S(X), k)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2092ca75-8689-485b-a880-ee6ceed46ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8893e-04],\n",
       "        [ 2.1037e-03],\n",
       "        [ 1.9999e+00]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YHat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee45ad4-8d0f-47c5-b613-2544f7a857b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41647b38-ef9f-4e01-bf7a-946169c39562",
   "metadata": {},
   "source": [
    "Now we take the next step.  We want to have a simple non-linear activation function.  The simplest\n",
    "being something like\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
