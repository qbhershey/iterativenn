{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email to Anura\n",
    " I have been reading some about Graph Neural Networks, and one of the things\n",
    "I have been trying to get my head wrapped around is what has been done before.  \n",
    "Along those lines, I think we need to really understand our work in the context of:\n",
    "\n",
    "http://snap.stanford.edu/node2vec/\n",
    "\n",
    "This is a classic and foundational work in GNNs and quite similiar (to my eye) to our\n",
    "approach.    I would like to hear what you think.\n",
    "\n",
    "  In particular, if you look at the corresponding paper (which has more than 9000 citations ðŸ™‚,\n",
    "they say:\n",
    "\n",
    "However, current techniques fail to satisfactorily define and opti-\n",
    "mize a reasonable objective required for scalable unsupervised fea-\n",
    "ture learning in networks. Classic approaches based on linear and\n",
    "non-linear dimensionality reduction techniques such as Principal\n",
    "Component Analysis, Multi-Dimensional Scaling and their exten-\n",
    "sions [3, 27, 30, 35] optimize an objective that transforms a repre-\n",
    "sentative data matrix of the network such that it maximizes the vari-\n",
    "ance of the data representation. Consequently, these approaches in-\n",
    "variably involve eigendecomposition of the appropriate data matrix\n",
    "which is expensive for large real-world networks. Moreover, the\n",
    "resulting latent representations give poor performance on various\n",
    "prediction tasks over networks.\n",
    "\n",
    "This is very close to what we propose (I believe), and appears to be more well studied\n",
    "than I had hoped.\n",
    "\n",
    "  I think that Zheyi is running into the idea that \"these approaches in-\n",
    "variably involve eigendecomposition of the appropriate data matrix\n",
    "which is expensive for large real-world networks\".  On the other hand,\n",
    "we believe that the statement  \"the resulting latent representations give\n",
    "poor performance on various prediction tasks over networks\" is actually\n",
    "not true.   \n",
    "  So, I think we need to address this in any proposal or paper we write.\n",
    "Note, I think there is a slick way around this.  I mean, when they say\n",
    "\"these approaches invariably involve eigendecomposition of the\n",
    "appropriate data matrixwhich is expensive for large real-world networks\" I\n",
    "don't think they take into account the matrix completion on distance matrix\n",
    "ideas that we have previously published.\n",
    "  I mean, there are papers like this:\n",
    "\n",
    "Inductive Matrix Completion Using Graph Autoencoder\n",
    "https://arxiv.org/pdf/2108.11124.pdf\n",
    "\n",
    "where distances are mentioned, but in a seemingly add-hoc way.\n",
    "  Another bit of good news is that dynamic graphs are far less well studied.  Here\n",
    "is a few months old survey:\n",
    "\n",
    "https://arxiv.org/pdf/2207.10128.pdf\n",
    "\n",
    "I think we have a lot to offer here.  I mean, our previous work on matrix completion\n",
    "again fits quite well here.  In particlar, suppose we have a hop distance matrix which\n",
    "is partially observed for which we already have the virtual coordinates/singular vectors.\n",
    "It is very fast to update the previous virtual coordinates/singular vectors if someone\n",
    "observes a few new distances.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
