{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torch\n",
    "\n",
    "from iterativenn.utils.DataModules import MNISTRepeatedSequenceDataModule\n",
    "from iterativenn.nn_modules.Sequential2D import Sequential2D\n",
    "from iterativenn.lit_modules import IteratedModel\n",
    "from iterativenn.utils.logger_factory import LoggerFacade\n",
    "\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I want to make some sample code where I validation a Sequential2D with as little jiggery-pokey as possible. I want it to look as close to standard pytorch code as possible. I want to be able to use the same code for validationing a Sequential2D as I would for training a standard pytorch model.  In particular, I want pytorch lightning to work with it in the same way it works with standard pytorch models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I want to be able to create a model from a configuration dictionary that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is the responsibility of each block given to the Sequential2D to take as input a vector of the right dimension and return a vector of the right dimension.\n",
    "# This can be checked and enforced by the 2d sequential, but it is the responsibility of the block to do the right thing.\n",
    "\n",
    "# It the responsibility of the factory to make sure that the blocks are compatible with each other.  This is a bit tricky, because some blocks take multi-dimensional\n",
    "# tensors as input.  The factory should be able to check that the input and output dimensions are compatible, and that the number of channels is compatible.\n",
    "\n",
    "# 'Input' blocks should be on the diagonal, but I guess that is not strictly necessary.  I mean, a non-diagonal block could be an input block, but it would be a bit strange in that it would\n",
    "# overwrite some other blocks output.  I guess that is not a problem, but it is a bit strange.\n",
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features\": [784, 200, 10], # 784 + 200 + 10 = 994\n",
    "        \"out_features\": [784, 140, 50, 20], # = # 784 + 140 + 50 + 20 = 994\n",
    "        \"block_types\": [\n",
    "            ['Input', None, None, None],\n",
    "            ['LSTM', 'Linear', 'MaskedLinear', None],\n",
    "            ['MaskedLinear.from_description', 'MaskedLinear.from_description', None, 'Conv1D'],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [{'type':'MNIST'}, None, None],\n",
    "            [{'LSTM_arg':'LSTM_value'}, None, None, None],\n",
    "            [{'block_type':'S=15', 'initialization_type':'G=0.2,0.7', 'trainable':True},\n",
    "             {'block_type':'S', 'initialization_type':'C=0.3', 'trainable':'non-zero'}, \n",
    "             None, {'Conv1D_arg':'Conv1D_value'}]\n",
    "        ],\n",
    "        \"start_y_index\":1 # Note correct... how to do?\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_epochs = 10\n",
    "global_optimizer = 'SGD'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for factory for the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this code is now in iterativenn/src/iterativenn/nn_modules/Sequential2D.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I want to create a factory that generates the model of interest from a configuration dictionary. To give us a starting point, let's consider the following linear operator\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "  784 \\times 784 & 784 \\times 200 & 784 \\times 10 \\\\\n",
    "  140 \\times 784 & 140 \\times 200 & 140 \\times 10  \\\\\n",
    "  50 \\times 784 & 50 \\times 200 & 50 \\times 10  \\\\\n",
    "  20 \\times 784 & 20 \\times 200 & 20 \\times 10  \\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{994 \\times 994}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X A^T = (A X^T)^T\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "which has the following form when thinking in terms of functions\n",
    "\n",
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "  f_{0,0}: 784 \\rightarrow 784 & f_{1,0}: 200 \\rightarrow 784 & f_{2,0}: 10 \\rightarrow 784 \\\\\n",
    "  f_{0,1}: 784 \\rightarrow 140 & f_{1,1}: 200 \\rightarrow 140 & f_{2,1}: 10 \\rightarrow 140 \\\\\n",
    "  f_{0,2}: 784 \\rightarrow 50  & f_{1,2}: 200 \\rightarrow 50  & f_{2,2}: 10 \\rightarrow 50  \\\\\n",
    "  f_{0,3}: 784 \\rightarrow 20  & f_{1,3}: 200 \\rightarrow 20  & f_{2,3}: 10 \\rightarrow 20  \\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{894 \\times 894}\n",
    "$$\n",
    "\n",
    "and\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each row of $X$ is the concatenation of some $x$ (input), $h$ (hidden), and $y$ output columns.  What makes some columns special?\n",
    "\n",
    "- $x$ columns are set by the outside world. They are the input to the model. \n",
    "- $h$ columns are set by the model. They are the hidden state of the model and don't really matter to anyone except the model.\n",
    "- $y$ columns are set by the model. They are the output of the model and are the only thing that matters to the outside world.  They feed back into the model by way of back-propagation.\n",
    "\n",
    "One way to handle $x$ is to make a special \n",
    "\n",
    "$$\n",
    "F^T = \\begin{bmatrix}\n",
    "  f_{0,0}: 784 \\rightarrow 784 & f_{0,1}: 784 \\rightarrow 140 & f_{0,2}: 784 \\rightarrow 50 & f_{0,3}: 784 \\rightarrow 20\\\\\n",
    "  f_{1,0}: 200 \\rightarrow 784 & f_{1,1}: 200 \\rightarrow 140 & f_{1,2}: 200 \\rightarrow 50 & f_{1,3}: 200 \\rightarrow 20\\\\\n",
    "  f_{2,0}: 10  \\rightarrow 784 & f_{2,1}: 10  \\rightarrow 140 & f_{2,2}: 10  \\rightarrow 50 & f_{2,3}: 10  \\rightarrow 20\\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{894 \\times 894}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements this idea.\n",
    "\n",
    "```python\n",
    "in_features_list = [784, 200, 10]\n",
    "out_features_list = [784, 140, 50, 20]\n",
    "blocks = [\n",
    "          [MaskedLinear(784, 784), MaskedLinear(784, 140), MaskedLinear(784, 50), MaskedLinear(784, 20)], \n",
    "          [MaskedLinear(200, 784), MaskedLinear(200, 140), MaskedLinear(200, 50), MaskedLinear(200, 20)], \n",
    "          [MaskedLinear(10,  784), MaskedLinear(10,  140), MaskedLinear(10,  50), MaskedLinear(10,  20)], \n",
    "         ]\n",
    "model = Sequential2D(in_features_list, out_features_list, blocks)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My model using factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factory_run(cfg, name):\n",
    "    log_name = name\n",
    "    logger = TensorBoardLogger(\"outputs\", name=log_name, version='main')\n",
    "    logger = LoggerFacade(logger, 'tensorboard', 'info')\n",
    "    sequential2D = Sequential2D.from_config(cfg[\"sequential2D\"])\n",
    "    callbacks = IteratedModel.ConfigCallbacks(cfg[\"callbacks\"])\n",
    "    model = IteratedModel.IteratedModel(sequential2D, \n",
    "                                        callbacks,\n",
    "                                        optimizer=global_optimizer)\n",
    "    data_module = MNISTRepeatedSequenceDataModule(min_copies=2, max_copies=2, seed=1234)\n",
    "    # This can be used to remove all of the extra outut from the training\n",
    "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "    # Initialize a trainer\n",
    "    trainer = Trainer(\n",
    "        accelerator='auto',\n",
    "        devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "        max_epochs=global_max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=False,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data_module.prepare_data()\n",
    "        data_module.setup('fit')\n",
    "        batch = next(iter(data_module.train_dataloader()))\n",
    "        loss = model.training_step(batch, 0, do_logging=False)\n",
    "        print(f\"loss before training: {loss}\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        # There are warning that I dont' care about at this moment and are not relevant to the example.\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        trainer.fit(model, data_module)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data_module.prepare_data()\n",
    "        data_module.setup('fit')\n",
    "        batch = next(iter(data_module.train_dataloader()))\n",
    "        loss = model.training_step(batch, 0, do_logging=False)\n",
    "        print(f\"loss after training: {loss}\")\n",
    "\n",
    "    return sequential2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features_list\": [28*28, 100, 10], \n",
    "        \"out_features_list\": [28*28, 100, 10], \n",
    "        \"block_types\": [\n",
    "            [None, 'Linear', None],\n",
    "            [None, None, 'Linear'],\n",
    "            [None, None, None],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [None, None, None],\n",
    "            [None, None, None],\n",
    "            [None, None, None],\n",
    "        ]\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"loss\": {\n",
    "            \"func\": \"CrossEntropyLoss\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10),\n",
    "            \"sequence_position\": 'last',\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"func\": \"zeros\",\n",
    "            \"size\": 28*28+100+10,\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"func\": \"insert\",\n",
    "            \"idx_list\": range(28*28),\n",
    "            \"flatten_input\": True,            \n",
    "        },\n",
    "        \"output\": {\n",
    "            \"func\": \"max\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10)\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 2.299802303314209\n",
      "loss after training: 1.892664909362793\n"
     ]
    }
   ],
   "source": [
    "previous_model = factory_run(cfg, \"factory_MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(previous_model, \"previous_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial growing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_model = torch.load(\"previous_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features_list\": [28*28+100+10, 10], \n",
    "        \"out_features_list\": [28*28+100+10, 10], \n",
    "        \"block_types\": [\n",
    "            ['Module', None],\n",
    "            [None, None],\n",
    "        ],\n",
    "\n",
    "        \"block_kwargs\": [\n",
    "            [{'module':previous_model}, None],\n",
    "            [None, None],\n",
    "        ],\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"loss\": {\n",
    "            \"func\": \"CrossEntropyLoss\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10),\n",
    "            \"sequence_position\": 'last',\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"func\": \"zeros\",\n",
    "            \"size\": 28*28+100+10+10,\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"func\": \"insert\",\n",
    "            \"idx_list\": range(28*28),\n",
    "            \"flatten_input\": True,\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"func\": \"max\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10)\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 1.892664909362793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.5198802947998047\n"
     ]
    }
   ],
   "source": [
    "tmp_model = factory_run(cfg, \"grow_MLP\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trivial but non-trainable growing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_model = torch.load(\"previous_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_block_kwargs = {'block_type':'W', 'initialization_type':'C=0.0', 'trainable':False, 'bias':False}\n",
    "\n",
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features_list\": [28*28+100+10, 10], \n",
    "        \"out_features_list\": [28*28+100+10, 10], \n",
    "        \"block_types\": [\n",
    "            ['Module', 'MaskedLinear.from_description'],\n",
    "            ['MaskedLinear.from_description', 'MaskedLinear.from_description'],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [{'module':previous_model}, default_block_kwargs],\n",
    "            [default_block_kwargs, default_block_kwargs],\n",
    "        ],\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"loss\": {\n",
    "            \"func\": \"CrossEntropyLoss\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10),\n",
    "            \"sequence_position\": 'last',\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"func\": \"zeros\",\n",
    "            \"size\": 28*28+100+10+10,\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"func\": \"insert\",\n",
    "            \"idx_list\": range(28*28),\n",
    "            \"flatten_input\": True,\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"func\": \"max\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 1.892664909362793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.5198802947998047\n"
     ]
    }
   ],
   "source": [
    "tmp_model = factory_run(cfg, \"grow_MLP_no_train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trivial and trainable growing the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_model = torch.load(\"previous_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_block_kwargs = {'block_type':'W', 'initialization_type':'G=0.0,0.0', 'trainable':True, 'bias':False}\n",
    "\n",
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features_list\": [28*28+100+10, 10], \n",
    "        \"out_features_list\": [28*28+100+10, 10], \n",
    "        \"block_types\": [\n",
    "            ['Module', 'MaskedLinear.from_description'],\n",
    "            ['MaskedLinear.from_description', 'MaskedLinear.from_description'],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [{'module':previous_model}, default_block_kwargs],\n",
    "            [default_block_kwargs, default_block_kwargs],\n",
    "        ],\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"loss\": {\n",
    "            \"func\": \"CrossEntropyLoss\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10),\n",
    "            \"sequence_position\": 'last',\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"func\": \"zeros\",\n",
    "            \"size\": 28*28+100+10+10,\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"func\": \"insert\",\n",
    "            \"idx_list\": range(28*28),\n",
    "            \"flatten_input\": True,\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"func\": \"max\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 1.892664909362793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.5198802947998047\n"
     ]
    }
   ],
   "source": [
    "tmp_model = factory_run(cfg, \"grow_MLP_train_init_std_0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_model = torch.load(\"previous_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_block_kwargs = {'block_type':'W', 'initialization_type':'G=0.0,0.1', 'trainable':True, 'bias':False}\n",
    "\n",
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features_list\": [28*28+100+10, 10], \n",
    "        \"out_features_list\": [28*28+100+10, 10], \n",
    "        \"block_types\": [\n",
    "            ['Module', 'MaskedLinear.from_description'],\n",
    "            ['MaskedLinear.from_description', 'MaskedLinear.from_description'],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [{'module':previous_model}, default_block_kwargs],\n",
    "            [default_block_kwargs, default_block_kwargs],\n",
    "        ],\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"loss\": {\n",
    "            \"func\": \"CrossEntropyLoss\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10),\n",
    "            \"sequence_position\": 'last',\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"func\": \"zeros\",\n",
    "            \"size\": 28*28+100+10+10,\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"func\": \"insert\",\n",
    "            \"idx_list\": range(28*28),\n",
    "            \"flatten_input\": True,\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"func\": \"max\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 1.9122493267059326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.4130362272262573\n"
     ]
    }
   ],
   "source": [
    "tmp_model = factory_run(cfg, \"grow_MLP_train_init_std_0.1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization with Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_model = torch.load(\"previous_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model01 = torch.nn.Linear(28*28+100+10, 10, bias=False)\n",
    "    model01.weight.normal_(0.0, 0.1)\n",
    "    model10 = torch.nn.Linear(10, 28*28+100+10, bias=False)\n",
    "    model10.weight.normal_(0.0, 0.1)\n",
    "    model11 = torch.nn.Linear(10, 10, bias=False)\n",
    "    model11.weight.normal_(0.0, 0.1)\n",
    "\n",
    "cfg = {\n",
    "    \"sequential2D\": {\n",
    "        \"in_features_list\": [28*28+100+10, 10], \n",
    "        \"out_features_list\": [28*28+100+10, 10], \n",
    "        \"block_types\": [\n",
    "            ['Module', 'Module'],\n",
    "            ['Module', 'Module'],\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [{'module':previous_model}, {'module':model01}],\n",
    "            [{'module':model10}, {'module':model11}],\n",
    "        ],\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"loss\": {\n",
    "            \"func\": \"CrossEntropyLoss\",\n",
    "            \"idx_list\" : range(28*28+100-5, 28*28+100+10),\n",
    "            \"sequence_position\": 'last',\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"func\": \"zeros\",\n",
    "            \"size\": 28*28+100+10+10,\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"func\": \"insert\",\n",
    "            \"idx_list\": range(28*28),\n",
    "            \"flatten_input\": True,\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"func\": \"max\",\n",
    "            \"idx_list\" : range(28*28+100, 28*28+100+10)\n",
    "        },\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 2.8078207969665527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.5807777643203735\n"
     ]
    }
   ],
   "source": [
    "tmp_model = factory_run(cfg, \"grow_MLP_train_Linear\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91c69d281b806a3add6b161e23d9089dcb392788fdbe6cdd17c006940cab5b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
